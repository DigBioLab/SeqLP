{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: c:\\Users\\nilsh\\my_projects\\SeqLP\\jupyter_scripts\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "dir_project = os.path.dirname(os.getcwd())\n",
    "path = os.path.join(dir_project, 'src')\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path)\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "#os.chdir(os.path.join(dir_project, 'src'))\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from seqlp.visualize.supervised_ml import DataPipeline, SupervisedML\n",
    "from seqlp.use_model import AnalyseModel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_fasta(fasta_file):\n",
    "    sequences = []\n",
    "    headers = []\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        sequences.append(str(record.seq))\n",
    "        headers.append(record.id)\n",
    "    return sequences, headers\n",
    "\n",
    "from Bio.Seq import Seq\n",
    "def translate_nucleotide_to_amino_acid(nucleotide_sequence):\n",
    "    seq = Seq(nucleotide_sequence)\n",
    "    return str(seq.translate())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\Bio\\Seq.py:2880: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sequencing_report = pd.read_csv(r\"C:\\Users\\nilsh\\my_projects\\ExpoSeq\\my_experiments\\max_new\\sequencing_report.csv\",)\n",
    "sequencing_report = sequencing_report.groupby(\"Experiment\").head(300)\n",
    "sequencing_report[\"full_seq\"] = sequencing_report[\"targetSequences\"].apply(translate_nucleotide_to_amino_acid)\n",
    "sequences = sequencing_report[\"full_seq\"].tolist()\n",
    "model_path = r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\" ## you can also use hugginface paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "Analyse = AnalyseModel(model_string = r\"facebook/esm2_t6_8M_UR50D\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\jupyter_scripts\\max_instruction.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nilsh/my_projects/SeqLP/jupyter_scripts/max_instruction.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m embeddings \u001b[39m=\u001b[39m Analyse\u001b[39m.\u001b[39;49m_embed_sequences(sequences)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nilsh/my_projects/SeqLP/jupyter_scripts/max_instruction.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m np\u001b[39m.\u001b[39msavetxt(\u001b[39m\"\u001b[39m\u001b[39mnanobody_embeddings.csv\u001b[39m\u001b[39m\"\u001b[39m, embeddings)\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\src\\seqlp\\use_model.py:53\u001b[0m, in \u001b[0;36mAnalyseModel._embed_sequences\u001b[1;34m(self, sequences)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPlease provide the sequences in a list format\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mtype\u001b[39m(sequences[\u001b[39m0\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39mstr\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mPlease provide the sequences in a list of strings\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 53\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mModelSets\u001b[39m.\u001b[39;49m_get_embeddings_parallel(sequences)\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\src\\seqlp\\visualize\\load_model.py:70\u001b[0m, in \u001b[0;36mLoadModel._get_embeddings_parallel\u001b[1;34m(self, full_sequences, batch_size)\u001b[0m\n\u001b[0;32m     67\u001b[0m inputs \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems()}  \u001b[39m# Move inputs to device\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():  \u001b[39m# Disable gradient computation for inference\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[0;32m     71\u001b[0m     last_hidden_state \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlast_hidden_state\n\u001b[0;32m     72\u001b[0m     avg_seq \u001b[39m=\u001b[39m last_hidden_state\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\transformers\\models\\esm\\modeling_esm.py:914\u001b[0m, in \u001b[0;36mEsmModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    905\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    907\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m    908\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    909\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    912\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    913\u001b[0m )\n\u001b[1;32m--> 914\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m    915\u001b[0m     embedding_output,\n\u001b[0;32m    916\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    917\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    918\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    919\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m    920\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    921\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    922\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    923\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    924\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    925\u001b[0m )\n\u001b[0;32m    926\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    927\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\transformers\\models\\esm\\modeling_esm.py:619\u001b[0m, in \u001b[0;36mEsmEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    608\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    609\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[0;32m    610\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    616\u001b[0m         output_attentions,\n\u001b[0;32m    617\u001b[0m     )\n\u001b[0;32m    618\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 619\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    620\u001b[0m         hidden_states,\n\u001b[0;32m    621\u001b[0m         attention_mask,\n\u001b[0;32m    622\u001b[0m         layer_head_mask,\n\u001b[0;32m    623\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    624\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    625\u001b[0m         past_key_value,\n\u001b[0;32m    626\u001b[0m         output_attentions,\n\u001b[0;32m    627\u001b[0m     )\n\u001b[0;32m    629\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\transformers\\models\\esm\\modeling_esm.py:509\u001b[0m, in \u001b[0;36mEsmLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    498\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    499\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    506\u001b[0m ):\n\u001b[0;32m    507\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    508\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 509\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    510\u001b[0m         hidden_states,\n\u001b[0;32m    511\u001b[0m         attention_mask,\n\u001b[0;32m    512\u001b[0m         head_mask,\n\u001b[0;32m    513\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    514\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    515\u001b[0m     )\n\u001b[0;32m    516\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    518\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\transformers\\models\\esm\\modeling_esm.py:443\u001b[0m, in \u001b[0;36mEsmAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    433\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    434\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    441\u001b[0m ):\n\u001b[0;32m    442\u001b[0m     hidden_states_ln \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states)\n\u001b[1;32m--> 443\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    444\u001b[0m         hidden_states_ln,\n\u001b[0;32m    445\u001b[0m         attention_mask,\n\u001b[0;32m    446\u001b[0m         head_mask,\n\u001b[0;32m    447\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    448\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    449\u001b[0m         past_key_value,\n\u001b[0;32m    450\u001b[0m         output_attentions,\n\u001b[0;32m    451\u001b[0m     )\n\u001b[0;32m    452\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    453\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\transformers\\models\\esm\\modeling_esm.py:370\u001b[0m, in \u001b[0;36mEsmSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    367\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m attention_mask\n\u001b[0;32m    369\u001b[0m \u001b[39m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[1;32m--> 370\u001b[0m attention_probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49msoftmax(attention_scores, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    372\u001b[0m \u001b[39m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[39m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m    374\u001b[0m attention_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_probs)\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:1858\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1856\u001b[0m     dim \u001b[39m=\u001b[39m _get_softmax_dim(\u001b[39m\"\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1858\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msoftmax(dim)\n\u001b[0;32m   1859\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1860\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msoftmax(dim, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embeddings = Analyse._embed_sequences(sequences)\n",
    "np.savetxt(\"nanobody_embeddings.csv\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cLNTX_bind', 'cLNTX_non-bind', 'cLNTX_+-', 'cLNTX_++', 'aBGTX_+-',\n",
       "       'aBGTX_++', 'aBGTX_-+'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = sequencing_report[\"Experiment\"].tolist()\n",
    "sequencing_report[\"Experiment\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is for assigning a specific color from the cmap to the label because otherwise you would have to rearrange the input labels \n",
    "\n",
    "cmap = plt.get_cmap('Set1')\n",
    "\n",
    "# Number of colors in Set2\n",
    "n_colors = cmap.N\n",
    "\n",
    "# Retrieve each color from the colormap\n",
    "colors = [cmap(i / float(n_colors - 1)) for i in range(n_colors)]\n",
    "color_list = [colors[3], colors[8], colors[4], colors[6], colors[1], colors[2], colors[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance after reducing to 19 dimensions: 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\umap\\umap_.py:1945: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.596078431372549, 0.3058823529411765, 0.6392156862745098, 0.6), (0.6, 0.6, 0.6, 0.6), (1.0, 0.4980392156862745, 0.0, 0.6), (0.6509803921568628, 0.33725490196078434, 0.1568627450980392, 0.6), (0.21568627450980393, 0.49411764705882355, 0.7215686274509804, 0.6), (0.30196078431372547, 0.6862745098039216, 0.2901960784313726, 0.6), (0.8941176470588236, 0.10196078431372549, 0.10980392156862745, 0.6)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nilsh\\my_projects\\SeqLP\\src\\seqlp\\use_model.py:249: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n",
      "  self.ax.scatter(localX[\"UMAP_1\"], localX[\"UMAP_2\"], s = size_points, c = color_list[index], label = unique_label, alpha = alpha, cmap = cmap_string)\n",
      "c:\\Users\\nilsh\\my_projects\\SeqLP\\src\\seqlp\\use_model.py:249: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n",
      "  self.ax.scatter(localX[\"UMAP_1\"], localX[\"UMAP_2\"], s = size_points, c = color_list[index], label = unique_label, alpha = alpha, cmap = cmap_string)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UMAP_1</th>\n",
       "      <th>UMAP_2</th>\n",
       "      <th>labels</th>\n",
       "      <th>labels_factorized</th>\n",
       "      <th>sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.186884</td>\n",
       "      <td>9.686086</td>\n",
       "      <td>cLNTX_bind</td>\n",
       "      <td>0</td>\n",
       "      <td>GSIGAIHAMGWFRQAPGSQRELVATVTTTSDSTTYGESVKGRFTIS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.805194</td>\n",
       "      <td>18.563639</td>\n",
       "      <td>cLNTX_bind</td>\n",
       "      <td>0</td>\n",
       "      <td>RDVGSIHAMAWYRQAPGKQRELVASVTMVGGETPTYAESIKGRFTI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.782341</td>\n",
       "      <td>2.031072</td>\n",
       "      <td>cLNTX_bind</td>\n",
       "      <td>0</td>\n",
       "      <td>GDISRINTMGWYRQAPGKQRELVADMSADTSATYADSVKGRFTVSR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.160986</td>\n",
       "      <td>12.320618</td>\n",
       "      <td>cLNTX_bind</td>\n",
       "      <td>0</td>\n",
       "      <td>GDISRINTMGWYRQAPGKQRELVADMLADTSATYANSVKGRFTVSR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.183554</td>\n",
       "      <td>14.509464</td>\n",
       "      <td>cLNTX_bind</td>\n",
       "      <td>0</td>\n",
       "      <td>GDISRINTMGWYRQAPGKQRELVADMLADASAKYADSVEGRFTVSR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>13.938734</td>\n",
       "      <td>11.744174</td>\n",
       "      <td>aBGTX_-+</td>\n",
       "      <td>6</td>\n",
       "      <td>GSIGSIYAMGWYRQAAGKQRELVANITTGGRATYSESVQGRFTISR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>11.351970</td>\n",
       "      <td>9.993547</td>\n",
       "      <td>aBGTX_-+</td>\n",
       "      <td>6</td>\n",
       "      <td>GSIFSSINNMAWYRQAPGKQRELVAGITGGGRPTYADSVKGRATIS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2097</th>\n",
       "      <td>12.660081</td>\n",
       "      <td>16.965624</td>\n",
       "      <td>aBGTX_-+</td>\n",
       "      <td>6</td>\n",
       "      <td>GSIGAIAAMGWYRQAPGKQRELVASVTMVGGETPTYAESIKGRFTI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2098</th>\n",
       "      <td>2.541236</td>\n",
       "      <td>17.107794</td>\n",
       "      <td>aBGTX_-+</td>\n",
       "      <td>6</td>\n",
       "      <td>RDVGSIHAMAWYRQAPGKQRELVATIGFGGRTDYADSVKDRFTISR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>12.195655</td>\n",
       "      <td>15.930380</td>\n",
       "      <td>aBGTX_-+</td>\n",
       "      <td>6</td>\n",
       "      <td>RDVGSIHAMAWYRQAPGKQRELVASITRDGITKYSESVQGRFTISS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         UMAP_1     UMAP_2      labels  labels_factorized  \\\n",
       "0     16.186884   9.686086  cLNTX_bind                  0   \n",
       "1     11.805194  18.563639  cLNTX_bind                  0   \n",
       "2      4.782341   2.031072  cLNTX_bind                  0   \n",
       "3      1.160986  12.320618  cLNTX_bind                  0   \n",
       "4      1.183554  14.509464  cLNTX_bind                  0   \n",
       "...         ...        ...         ...                ...   \n",
       "2095  13.938734  11.744174    aBGTX_-+                  6   \n",
       "2096  11.351970   9.993547    aBGTX_-+                  6   \n",
       "2097  12.660081  16.965624    aBGTX_-+                  6   \n",
       "2098   2.541236  17.107794    aBGTX_-+                  6   \n",
       "2099  12.195655  15.930380    aBGTX_-+                  6   \n",
       "\n",
       "                                              sequences  \n",
       "0     GSIGAIHAMGWFRQAPGSQRELVATVTTTSDSTTYGESVKGRFTIS...  \n",
       "1     RDVGSIHAMAWYRQAPGKQRELVASVTMVGGETPTYAESIKGRFTI...  \n",
       "2     GDISRINTMGWYRQAPGKQRELVADMSADTSATYADSVKGRFTVSR...  \n",
       "3     GDISRINTMGWYRQAPGKQRELVADMLADTSATYANSVKGRFTVSR...  \n",
       "4     GDISRINTMGWYRQAPGKQRELVADMLADASAKYADSVEGRFTVSR...  \n",
       "...                                                 ...  \n",
       "2095  GSIGSIYAMGWYRQAAGKQRELVANITTGGRATYSESVQGRFTISR...  \n",
       "2096  GSIFSSINNMAWYRQAPGKQRELVAGITGGGRPTYADSVKGRATIS...  \n",
       "2097  GSIGAIAAMGWYRQAPGKQRELVASVTMVGGETPTYAESIKGRFTI...  \n",
       "2098  RDVGSIHAMAWYRQAPGKQRELVATIGFGGRTDYADSVKDRFTISR...  \n",
       "2099  RDVGSIHAMAWYRQAPGKQRELVASITRDGITKYSESVQGRFTISS...  \n",
       "\n",
       "[2100 rows x 5 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = \"Embeddings of Nanobody sequences with UMAP\"\n",
    "Analyse.embed_cluster_label(sequences, labels, n_neighbors = 15, min_dist=0.1, alpha = 0.6, size_points = 30, cmap = \"Set1\", color_list = color_list, title = title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analyse.save_in_plots(\"nanobody_umap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.loadtxt(\"nanobody_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "title = \"Embeddings of Nanobody sequences with UMAP\"\n",
    "Analyse.embed_cluster_label(sequences, labels, embeddings = embeddings, n_neighbors = 20, min_dist = 0.01, alpha = 0.6, size_points = 20, cmap = \"Set1\", color_list = color_list, title = title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analyse.save_in_plots(\"nanobody_umap\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
