{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and important functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: c:\\Users\\nilsh\\my_projects\\SeqLP\\jupyter_scripts\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "dir_project = os.path.dirname(os.getcwd())\n",
    "path = os.path.join(dir_project, 'src')\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path)\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "#os.chdir(os.path.join(dir_project, 'src'))\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from seqlp.visualize.supervised_ml import DataPipeline, SupervisedML\n",
    "import glob\n",
    "from seqlp.use_model import AnalyseModel\n",
    "from Bio import SeqIO\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nanobody_delphia_no_ag_filter = pd.read_excel(r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\data\\intermediate_tables\\nanobody_delphia_no_ag_filter.xlsx\")\n",
    "nanobody_delphia_ag_filter = pd.read_excel(r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\data\\intermediate_tables\\nanobody_delphia_tidied.xlsx\")\n",
    "binding_data = pd.read_excel(r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\data\\all_binding_data.xlsx\")\n",
    "sequencing_report = pd.read_csv(r\"C:\\Users\\nilsh\\my_projects\\ExpoSeq\\my_experiments\\max_new\\sequencing_report.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cloneId</th>\n",
       "      <th>readCount</th>\n",
       "      <th>readFraction</th>\n",
       "      <th>targetSequences</th>\n",
       "      <th>targetQualities</th>\n",
       "      <th>nSeqFR1</th>\n",
       "      <th>minQualFR1</th>\n",
       "      <th>nSeqCDR1</th>\n",
       "      <th>minQualCDR1</th>\n",
       "      <th>nSeqFR2</th>\n",
       "      <th>...</th>\n",
       "      <th>nSeqFR4</th>\n",
       "      <th>minQualFR4</th>\n",
       "      <th>aaSeqFR1</th>\n",
       "      <th>aaSeqCDR1</th>\n",
       "      <th>aaSeqFR2</th>\n",
       "      <th>aaSeqCDR2</th>\n",
       "      <th>aaSeqFR3</th>\n",
       "      <th>aaSeqCDR3</th>\n",
       "      <th>aaSeqFR4</th>\n",
       "      <th>cloneFraction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Experiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1_cLNTX_bind</th>\n",
       "      <td>5683</td>\n",
       "      <td>5683</td>\n",
       "      <td>5683</td>\n",
       "      <td>5683</td>\n",
       "      <td>5683</td>\n",
       "      <td>5683</td>\n",
       "      <td>5683</td>\n",
       "      <td>5683</td>\n",
       "      <td>5683</td>\n",
       "      <td>5683</td>\n",
       "      <td>...</td>\n",
       "      <td>5683</td>\n",
       "      <td>5683</td>\n",
       "      <td>5683</td>\n",
       "      <td>5683</td>\n",
       "      <td>5683</td>\n",
       "      <td>5681</td>\n",
       "      <td>5683</td>\n",
       "      <td>5683</td>\n",
       "      <td>5683</td>\n",
       "      <td>5683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_cLNTX_non-bind</th>\n",
       "      <td>4843</td>\n",
       "      <td>4843</td>\n",
       "      <td>4843</td>\n",
       "      <td>4843</td>\n",
       "      <td>4843</td>\n",
       "      <td>4843</td>\n",
       "      <td>4843</td>\n",
       "      <td>4842</td>\n",
       "      <td>4843</td>\n",
       "      <td>4842</td>\n",
       "      <td>...</td>\n",
       "      <td>4843</td>\n",
       "      <td>4843</td>\n",
       "      <td>4843</td>\n",
       "      <td>4842</td>\n",
       "      <td>4842</td>\n",
       "      <td>4842</td>\n",
       "      <td>4843</td>\n",
       "      <td>4843</td>\n",
       "      <td>4843</td>\n",
       "      <td>4843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_cLNTX+-</th>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>...</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_cLNTX_++</th>\n",
       "      <td>3204</td>\n",
       "      <td>3204</td>\n",
       "      <td>3204</td>\n",
       "      <td>3204</td>\n",
       "      <td>3204</td>\n",
       "      <td>3204</td>\n",
       "      <td>3204</td>\n",
       "      <td>3204</td>\n",
       "      <td>3204</td>\n",
       "      <td>3204</td>\n",
       "      <td>...</td>\n",
       "      <td>3204</td>\n",
       "      <td>3204</td>\n",
       "      <td>3204</td>\n",
       "      <td>3204</td>\n",
       "      <td>3204</td>\n",
       "      <td>3203</td>\n",
       "      <td>3204</td>\n",
       "      <td>3204</td>\n",
       "      <td>3204</td>\n",
       "      <td>3204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_aBGXT+-</th>\n",
       "      <td>468</td>\n",
       "      <td>468</td>\n",
       "      <td>468</td>\n",
       "      <td>468</td>\n",
       "      <td>468</td>\n",
       "      <td>468</td>\n",
       "      <td>468</td>\n",
       "      <td>468</td>\n",
       "      <td>468</td>\n",
       "      <td>468</td>\n",
       "      <td>...</td>\n",
       "      <td>468</td>\n",
       "      <td>468</td>\n",
       "      <td>468</td>\n",
       "      <td>468</td>\n",
       "      <td>468</td>\n",
       "      <td>468</td>\n",
       "      <td>468</td>\n",
       "      <td>468</td>\n",
       "      <td>468</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_aBGTX_++</th>\n",
       "      <td>1360</td>\n",
       "      <td>1360</td>\n",
       "      <td>1360</td>\n",
       "      <td>1360</td>\n",
       "      <td>1360</td>\n",
       "      <td>1360</td>\n",
       "      <td>1360</td>\n",
       "      <td>1360</td>\n",
       "      <td>1360</td>\n",
       "      <td>1360</td>\n",
       "      <td>...</td>\n",
       "      <td>1360</td>\n",
       "      <td>1360</td>\n",
       "      <td>1360</td>\n",
       "      <td>1360</td>\n",
       "      <td>1360</td>\n",
       "      <td>1360</td>\n",
       "      <td>1360</td>\n",
       "      <td>1360</td>\n",
       "      <td>1360</td>\n",
       "      <td>1360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7_aBGTX-+</th>\n",
       "      <td>904</td>\n",
       "      <td>904</td>\n",
       "      <td>904</td>\n",
       "      <td>904</td>\n",
       "      <td>904</td>\n",
       "      <td>904</td>\n",
       "      <td>904</td>\n",
       "      <td>904</td>\n",
       "      <td>904</td>\n",
       "      <td>904</td>\n",
       "      <td>...</td>\n",
       "      <td>904</td>\n",
       "      <td>904</td>\n",
       "      <td>904</td>\n",
       "      <td>904</td>\n",
       "      <td>904</td>\n",
       "      <td>904</td>\n",
       "      <td>904</td>\n",
       "      <td>904</td>\n",
       "      <td>904</td>\n",
       "      <td>904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  cloneId  readCount  readFraction  targetSequences  \\\n",
       "Experiment                                                            \n",
       "1_cLNTX_bind         5683       5683          5683             5683   \n",
       "2_cLNTX_non-bind     4843       4843          4843             4843   \n",
       "3_cLNTX+-             540        540           540              540   \n",
       "4_cLNTX_++           3204       3204          3204             3204   \n",
       "5_aBGXT+-             468        468           468              468   \n",
       "6_aBGTX_++           1360       1360          1360             1360   \n",
       "7_aBGTX-+             904        904           904              904   \n",
       "\n",
       "                  targetQualities  nSeqFR1  minQualFR1  nSeqCDR1  minQualCDR1  \\\n",
       "Experiment                                                                      \n",
       "1_cLNTX_bind                 5683     5683        5683      5683         5683   \n",
       "2_cLNTX_non-bind             4843     4843        4843      4842         4843   \n",
       "3_cLNTX+-                     540      540         540       540          540   \n",
       "4_cLNTX_++                   3204     3204        3204      3204         3204   \n",
       "5_aBGXT+-                     468      468         468       468          468   \n",
       "6_aBGTX_++                   1360     1360        1360      1360         1360   \n",
       "7_aBGTX-+                     904      904         904       904          904   \n",
       "\n",
       "                  nSeqFR2  ...  nSeqFR4  minQualFR4  aaSeqFR1  aaSeqCDR1  \\\n",
       "Experiment                 ...                                             \n",
       "1_cLNTX_bind         5683  ...     5683        5683      5683       5683   \n",
       "2_cLNTX_non-bind     4842  ...     4843        4843      4843       4842   \n",
       "3_cLNTX+-             540  ...      540         540       540        540   \n",
       "4_cLNTX_++           3204  ...     3204        3204      3204       3204   \n",
       "5_aBGXT+-             468  ...      468         468       468        468   \n",
       "6_aBGTX_++           1360  ...     1360        1360      1360       1360   \n",
       "7_aBGTX-+             904  ...      904         904       904        904   \n",
       "\n",
       "                  aaSeqFR2  aaSeqCDR2  aaSeqFR3  aaSeqCDR3  aaSeqFR4  \\\n",
       "Experiment                                                             \n",
       "1_cLNTX_bind          5683       5681      5683       5683      5683   \n",
       "2_cLNTX_non-bind      4842       4842      4843       4843      4843   \n",
       "3_cLNTX+-              540        540       540        540       540   \n",
       "4_cLNTX_++            3204       3203      3204       3204      3204   \n",
       "5_aBGXT+-              468        468       468        468       468   \n",
       "6_aBGTX_++            1360       1360      1360       1360      1360   \n",
       "7_aBGTX-+              904        904       904        904       904   \n",
       "\n",
       "                  cloneFraction  \n",
       "Experiment                       \n",
       "1_cLNTX_bind               5683  \n",
       "2_cLNTX_non-bind           4843  \n",
       "3_cLNTX+-                   540  \n",
       "4_cLNTX_++                 3204  \n",
       "5_aBGXT+-                   468  \n",
       "6_aBGTX_++                 1360  \n",
       "7_aBGTX-+                   904  \n",
       "\n",
       "[7 rows x 27 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequencing_report.groupby(\"Experiment\").count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nanobody_model = r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\"\n",
    "esm_small = r\"facebook/esm2_t6_8M_UR50D\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_fasta(fasta_file):\n",
    "    sequences = []\n",
    "    headers = []\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        sequences.append(str(record.seq))\n",
    "        headers.append(record.id)\n",
    "    return sequences, headers\n",
    "\n",
    "from Bio.Seq import Seq\n",
    "def translate_nucleotide_to_amino_acid(nucleotide_sequence):\n",
    "    seq = Seq(nucleotide_sequence)\n",
    "    return str(seq.translate())    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = DataPipeline(model = r\"C:\\Users\\nilsh\\my_projects\\ExpoSeq\\models\\nanobody_full\",\n",
    "             path_seq_report = r\"C:\\Users\\nilsh\\my_projects\\ExpoSeq\\my_experiments\\max_new\\sequencing_report.csv\",\n",
    "             no_sequences = 100000,  # take basically all sequences\n",
    "             pca_components=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencing_report = Data.init_sequencing_report\n",
    "sequencing_report[\"v_gene\"] = sequencing_report['allVHitsWithScore'].str.split('*').str[0]\n",
    "experiments = sequencing_report[\"Experiment\"].unique().tolist()\n",
    "v_family = sequencing_report[\"v_gene\"].tolist()\n",
    "sequencing_report[\"full_seq\"] = Data.full_sequences\n",
    "print(f\"No. of sequences in report: {sequencing_report.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(r'c:\\Users\\nilsh\\OneDrive\\Desktop\\master_thesis\\train_model\\concatenatednanobody_full_train.csv.gz', compression='gzip')\n",
    "sequences = training_data.iloc[:, 0]\n",
    "sequences = sequences.str.replace(' ', '')\n",
    "print(f\"No. of training sequences: {training_data.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = sequencing_report['full_seq'].isin(sequences)\n",
    "\n",
    "# Step 4: Filter the DataFrame\n",
    "sequencing_report = sequencing_report[~mask]\n",
    "print(f\"No. of sequences in report after filtering training sequences: {sequencing_report.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity Analysis\n",
    "\n",
    "The goal is to find a metric which captures how decisive or confused the model is. We will use perplexity for this which is 2 ** Entropy.\n",
    "The highest entropy for this task is 20. That means that each amino acid per position can appear with equal probability which is very bad.\n",
    "\n",
    "Cons of this metric:\n",
    "- not good for final evaluation, since it just measures the model's confidence not its accuracy\n",
    "- A model with a lower perplexity can still be worse because it can just be very decisive but then in the clsutering would kinda fail to represent the sequences meaningfully.\n",
    "\n",
    "-> your model does not output logits, so it does not really make sense to do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_perplexity(perplexities):\n",
    "    mean_perplexity = np.mean(perplexities)\n",
    "    median_perplexity = np.median(perplexities)\n",
    "    std_dev = np.std(perplexities)\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(perplexities, bins=30, alpha=0.7, color='blue', label='Perplexity')\n",
    "    plt.axvline(mean_perplexity, color='r', linestyle='dashed', linewidth=1, label=f'Mean: {mean_perplexity:.2f}')\n",
    "    plt.axvline(median_perplexity, color='g', linestyle='dashed', linewidth=1, label=f'Median: {median_perplexity:.2f}')\n",
    "    plt.title('Distribution of Perplexity Scores')\n",
    "    plt.xlabel('Perplexity')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqlp.visualize.load_model import LoadModel\n",
    "\n",
    "NanobodyModel = LoadModel(r\"C:\\Users\\nilsh\\my_projects\\ExpoSeq\\models\\nanobody_full\")\n",
    "sequences = sequencing_report[\"full_seq\"].tolist()\n",
    "perplexities = []\n",
    "for sequence in sequences:\n",
    "    perplexity = NanobodyModel._get_perplexity(sequence)\n",
    "    perplexities.append(perplexity)\n",
    "perplexities = np.array(perplexities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COmpare model architectures based on no of components to reach 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = pd.read_csv(r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\data\\training_data\\val.csv\", nrows = 100000)[\"sequence\"].tolist()\n",
    "Analyse = AnalyseModel(r\"c:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\models\\t6_320_lastlayer_2106_seqs\")\n",
    "n_comp_collected = []\n",
    "sequence_lengths = [100, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "\n",
    "for length in sequence_lengths:\n",
    "    chunk = sequences[:length]\n",
    "    n_comp_collected.append(Analyse.no_components_for_sequences(chunk))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for clustering\n",
    "\n",
    "- Show how model clusters nanobodies with binding data. This model should do that gradually while the others should have problems with that\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse different layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_sequences, experiments = DataPipeline.wrangle_report(sequencing_report)\n",
    "import numpy as np\n",
    "full_sequences = np.array(full_sequences).reshape(-1, 12050).T\n",
    "experiments = np.array(experiments).reshape(-1, 12050).T\n",
    "\n",
    "full_sequences.shape\n",
    "\n",
    "# Create a DataFrame from the arrays\n",
    "df = pd.DataFrame({\n",
    "    \"CDR3\": sequencing_report[\"aaSeqCDR3\"],\n",
    "    'Full Sequences': full_sequences.flatten(),  # Flattening in case the array is 2D but should be 1D per column\n",
    "    'Experiments': experiments.flatten(),\n",
    "\n",
    "})\n",
    "df_filtered = df.drop_duplicates(subset = [\"CDR3\"], keep = \"last\")\n",
    "# This creates a mask that is True for rows where 'Full Sequences' does not contain 'region_not_covered'\n",
    "mask = ~df['Full Sequences'].str.contains('region_not_covered')\n",
    "\n",
    "# Apply the mask to the DataFrame to keep only the rows where the condition is True\n",
    "df_filtered = df[mask]\n",
    "\n",
    "df_filtered = df_filtered.groupby(\"Experiments\").head(200)\n",
    "df_filtered[\"Experiments\"].unique()\n",
    "experiments = df_filtered[\"Experiments\"].tolist()\n",
    "full_sequences = df_filtered[\"Full Sequences\"].tolist()\n",
    "cdr3 = df_filtered[\"CDR3\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('Set2')\n",
    "\n",
    "# Number of colors in Set2\n",
    "n_colors = cmap.N\n",
    "\n",
    "# Retrieve each color from the colormap\n",
    "colors = [cmap(i / float(n_colors - 1)) for i in range(n_colors)]\n",
    "color_list = [colors[0], colors[7], colors[1], colors[2], colors[3], colors[4], colors[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = [colors[0], colors[7], colors[1], colors[2], colors[3], colors[6], colors[5]]\n",
    "def sequence_label(model_path, picture_path, title_extension, sequences, labels,):\n",
    "    from transformers import RoFormerTokenizer, RoFormerModel\n",
    "    if model_path == \"alchemab/antiberta2-cssp\":\n",
    "        tokenizer = RoFormerTokenizer.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "        model = RoFormerModel.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "        Analyse = AnalyseModel(\"alchemab/antiberta2-cssp\", model, tokenizer )\n",
    "    else:\n",
    "        Analyse = AnalyseModel(model_path)\n",
    "    reduced_X = Analyse.embed_cluster_label(sequences, labels, explained_variance_threshold = 0.9,\n",
    "                                            size_points = 15, n_neighbors = 15, min_dist = 0.1, alpha = 0.8, \n",
    "                                            cmap = \"Set2\", title = title_extension, color_list = color_list)\n",
    "    reduced_X.to_csv(f\"{picture_path}.csv\")\n",
    "\n",
    "    Analyse.save_in_plots(f\"{picture_path}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = nanobody_delphia_ag_filter[\"Full sequence\"].tolist()\n",
    "labels = nanobody_delphia_ag_filter[\"Ag1_modified\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_label(r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\models\\t12_320_lastlayer_2106_seqs\", r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\validation_embedding\\measure_pca_little\", \n",
    "               \"6 attention layers and 320 nodes in last hidden layer\", sequences, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare binding data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed specific targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sequence_label(model_path, picture_path, title_extension):\n",
    "    signals = nanobody_delphia_ag_filter[\"Ag1_Raw signals\"].tolist()\n",
    "    labels = nanobody_delphia_ag_filter[\"Ag1_modified\"].tolist()\n",
    "    sequences = nanobody_delphia_ag_filter[\"Full sequence\"].tolist()\n",
    "    from transformers import RoFormerTokenizer, RoFormerModel\n",
    "    if model_path == \"alchemab/antiberta2-cssp\":\n",
    "        tokenizer = RoFormerTokenizer.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "        model = RoFormerModel.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "        Analyse = AnalyseModel(\"alchemab/antiberta2-cssp\", model, tokenizer )\n",
    "    else:\n",
    "        Analyse = AnalyseModel(model_path)\n",
    "    reduced_X = Analyse.embed_cluster_label(sequences, labels, explained_variance_threshold = 0.9,size_points = 50, n_neighbors = 15, min_dist = 0.1, alpha = 0.8, cmap = \"Set2\", title = f\"Embedding of nanobody sequences with {title_extension}\")\n",
    "    reduced_X[\"Name VHH\"] = nanobody_delphia_ag_filter[\"Name VHH\"]\n",
    "    reduced_X.to_csv(f\"{picture_path}.csv\")\n",
    "\n",
    "    Analyse.save_in_plots(f\"{picture_path}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_label(r\"facebook/esm2_t6_8M_UR50D\", r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\validation_embedding\\embedding_esm_t6_90%\", \"Esm-2b with 6 layers as baseline model\", )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_label(r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\", r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\validation_embedding\\embedding_self_trained_90%\", \"trained model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_label(r\"alchemab/antiberta2-cssp\", r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\validation_embedding\\embedding_antiberta_90%\", r\"Antiberta2\")\n",
    "sequence_label(r\"facebook/esm2_t6_8M_UR50D\", r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\validation_embedding\\embedding_esm_t6_90%\", \"Esm-2b with 6 layers as baseline model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_label(r\"facebook/esm2_t36_3B_UR50D\", r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\validation_embedding\\embedding_esm_t36_90%\", \"Esm-2b with 36 layers as baseline model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just embed whole venoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nanobody_delphia = nanobody_delphia_no_ag_filter.dropna(subset=['Ag1_modified'])\n",
    "print(nanobody_delphia[\"Ag1_modified\"].unique())\n",
    "print(nanobody_delphia.shape)\n",
    "nanobody_delphia = nanobody_delphia[nanobody_delphia[\"Ag1_modified\"] != \"SVSP\"]\n",
    "nanobody_delphia = nanobody_delphia[nanobody_delphia[\"Ag1_modified\"] != \"Dv4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('Set2')\n",
    "cmap_2 = plt.get_cmap('Set1')\n",
    "# Number of colors in Set2\n",
    "n_colors = cmap.N\n",
    "n_colors_2 = cmap_2.N\n",
    "# Retrieve each color from the colormap\n",
    "colors_2 = [cmap_2(i / float(n_colors_2 - 1)) for i in range(n_colors_2)]\n",
    "\n",
    "colors = [cmap(i / float(n_colors - 1)) for i in range(n_colors)]\n",
    "color_list = [colors_2[0], colors[5], colors_2[1], colors[2], colors[1],  colors[4], colors_2[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\"\n",
    "picture_path = \"all_venoms_self_trained_90%\"\n",
    "title_extension = \"Embedding of \"\n",
    "signals = nanobody_delphia[\"Ag1_Raw signals\"].tolist()\n",
    "labels = nanobody_delphia[\"Ag1_modified\"].tolist()\n",
    "sequences = nanobody_delphia[\"Full sequence\"].tolist()\n",
    "from transformers import RoFormerTokenizer, RoFormerModel\n",
    "if model_path == \"alchemab/antiberta2-cssp\":\n",
    "    tokenizer = RoFormerTokenizer.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "    model = RoFormerModel.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "    Analyse = AnalyseModel(\"alchemab/antiberta2-cssp\", model, tokenizer )\n",
    "else:\n",
    "    Analyse = AnalyseModel(model_path)\n",
    "reduced_X = Analyse.embed_cluster_label(sequences, labels, explained_variance_threshold = 0.9,size_points = 35, n_neighbors = 15, min_dist = 0.1, alpha = 0.9, cmap = \"Set2\", title = f\"Embedding of nanobody sequences selected against different venoms or toxins\", color_list = color_list)\n",
    "reduced_X.to_csv(f\"{picture_path}.csv\")\n",
    "\n",
    "Analyse.save_in_plots(f\"{picture_path}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('Set2')\n",
    "\n",
    "# Number of colors in Set2\n",
    "n_colors = cmap.N\n",
    "\n",
    "# Retrieve each color from the colormap\n",
    "colors = [cmap(i / float(n_colors - 1)) for i in range(n_colors)]\n",
    "color_list = [colors[0], colors[7], colors[1], colors[2], colors[3], colors[4], colors[5]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencing_report = pd.read_csv(r\"C:\\Users\\nilsh\\my_projects\\ExpoSeq\\my_experiments\\max_new\\sequencing_report.csv\",)\n",
    "sequencing_report.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencing_report.groupby(\"Experiment\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the arrays\n",
    "df = pd.DataFrame({\n",
    "    \"CDR3\": sequencing_report[\"aaSeqCDR3\"],\n",
    "    'Full Sequences': full_sequences.flatten(),  # Flattening in case the array is 2D but should be 1D per column\n",
    "    'Experiments': experiments.flatten(),\n",
    "\n",
    "})\n",
    "df_filtered = df.drop_duplicates(subset = [\"CDR3\"], keep = \"last\")\n",
    "# This creates a mask that is True for rows where 'Full Sequences' does not contain 'region_not_covered'\n",
    "mask = ~df['Full Sequences'].str.contains('region_not_covered')\n",
    "\n",
    "# Apply the mask to the DataFrame to keep only the rows where the condition is True\n",
    "df_filtered = df[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.groupby(\"Experiments\").head(200)\n",
    "df_filtered[\"Experiments\"].unique()\n",
    "experiments = df_filtered[\"Experiments\"].tolist()\n",
    "full_sequences = df_filtered[\"Full Sequences\"].tolist()\n",
    "cdr3 = df_filtered[\"CDR3\"].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "color_list = [colors[0], colors[7], colors[1], colors[2], colors[3], colors[6], colors[5]]\n",
    "def sequence_label(model_path, picture_path, title_extension, sequences, labels,):\n",
    "    from transformers import RoFormerTokenizer, RoFormerModel\n",
    "    if model_path == \"alchemab/antiberta2-cssp\":\n",
    "        tokenizer = RoFormerTokenizer.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "        model = RoFormerModel.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "        Analyse = AnalyseModel(\"alchemab/antiberta2-cssp\", model, tokenizer )\n",
    "    else:\n",
    "        Analyse = AnalyseModel(model_path)\n",
    "    reduced_X = Analyse.embed_cluster_label(sequences, labels, explained_variance_threshold = 0.9,\n",
    "                                            size_points = 15, n_neighbors = 15, min_dist = 0.1, alpha = 0.8, \n",
    "                                            cmap = \"Set3\", title = f\"Embedding of CDR3 sequences with {title_extension}\", color_list = color_list)\n",
    "    reduced_X.to_csv(f\"{picture_path}.csv\")\n",
    "\n",
    "    Analyse.save_in_plots(f\"{picture_path}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sequence_label(r\"facebook/esm2_t6_8M_UR50D\", r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\validation_embedding\\embedding_baseline_max_data_cdr3\", \n",
    "               \"Esm-2b with 6 layers as baseline model\", cdr3, experiments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_label(r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\", r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\validation_embedding\\embedding_nanobody_model_max_data_cdr3\", \n",
    "               \"trained model\", cdr3, experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Seq import Seq\n",
    "def translate_nucleotide_to_amino_acid(nucleotide_sequence):\n",
    "    seq = Seq(nucleotide_sequence)\n",
    "    return str(seq.translate())    \n",
    "\n",
    "\n",
    "model_path = r\"alchemab/antiberta2-cssp\"\n",
    "name_data = \"embedding_antiberta_non_binding\"\n",
    "nanobody_delphia = pd.read_excel(r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\jupyter_scripts\\antibody_tidied.xlsx\")\n",
    "consensus_binding = nanobody_delphia[nanobody_delphia[\"Ag1\"] == \"scNTX\"]\n",
    "sequences = consensus_binding[\"Full sequence\"].tolist()\n",
    "signals = consensus_binding[\"Ag1_Raw signals\"].tolist()\n",
    "faqs_data = pd.read_csv(r\"C:\\Users\\nilsh\\my_projects\\ExpoSeq\\my_experiments\\max_new\\sequencing_report.csv\")\n",
    "faqs_data = faqs_data.groupby(\"Experiment\").head(200)\n",
    "binder_unlabeled = faqs_data[faqs_data[\"Experiment\"] == \"cLNTX_bind\"]\n",
    "non_binder = faqs_data[faqs_data[\"Experiment\"] == \"cLNTX_non-bind\"]\n",
    "seq_binder = non_binder[\"targetSequences\"].apply(translate_nucleotide_to_amino_acid).tolist()\n",
    "seq_non_binder = non_binder[\"targetSequences\"].apply(translate_nucleotide_to_amino_acid).tolist()\n",
    "binding_values_non_binder = len(seq_non_binder) * [0]\n",
    "binding_values_binder = len(seq_binder) * [0]\n",
    "all_binding =  binding_values_binder +binding_values_non_binder + signals\n",
    "all_sequences =  seq_binder + seq_non_binder + sequences\n",
    "labels =  len(seq_binder) * [\"cLNTX binder\"] + len(binding_values_non_binder) * [\"Non Binder\"] + len(signals) * [\"cLNTX binder\"]\n",
    "\n",
    "from transformers import RoFormerTokenizer, RoFormerModel\n",
    "if model_path == \"alchemab/antiberta2-cssp\":\n",
    "    tokenizer = RoFormerTokenizer.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "    model = RoFormerModel.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "    Analyse = AnalyseModel(\"alchemab/antiberta2-cssp\", model, tokenizer )\n",
    "else:\n",
    "    Analyse = AnalyseModel(model_path)\n",
    "reduced_X = Analyse.embed_cluster_sequences(all_sequences, labels, all_binding, explained_variance_threshold = 0.9, n_neighbors = 15, min_dist = 0.1, alpha = 0.8, cmap = \"inferno\", title = \"Embedding of nanobody sequences with Antiberta 2.\")\n",
    "#reduced_X.to_csv(f\"{name_data}.csv\")\n",
    "\n",
    "Analyse.save_in_plots(f\"{name_data}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate with binding data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sequences, experiments = DataPipeline.wrangle_report(sequencing_report)\n",
    "import numpy as np\n",
    "full_sequences = np.array(full_sequences).reshape(-1, 12050).T\n",
    "experiments = np.array(experiments).reshape(-1, 12050).T\n",
    "\n",
    "full_sequences.shape\n",
    "df = pd.DataFrame({\n",
    "    \"CDR3\": sequencing_report[\"aaSeqCDR3\"],\n",
    "    'Full sequence': full_sequences.flatten(),  # Flattening in case the array is 2D but should be 1D per column\n",
    "    'Experiments': experiments.flatten(),\n",
    "\n",
    "})\n",
    "df_filtered = df.drop_duplicates(subset = [\"CDR3\"], keep = \"last\")\n",
    "# This creates a mask that is True for rows where 'Full Sequences' does not contain 'region_not_covered'\n",
    "mask = ~df['Full sequence'].str.contains('region_not_covered')\n",
    "\n",
    "# Apply the mask to the DataFrame to keep only the rows where the condition is True\n",
    "df_filtered = df[mask]\n",
    "df_filtered = df_filtered.groupby(\"Experiments\").head(200)\n",
    "bind_ngs = df_filtered.merge(binding_data, on = \"Full sequence\", how = \"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bind_ngs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_sequences = bind_ngs[\"Full sequence\"].tolist()\n",
    "bind_ngs = bind_ngs.fillna(0)\n",
    "signals = bind_ngs[[\"⍺-BuTx\", \"aCBTX\"]]\n",
    "aCBTX = bind_ngs[\"aCBTX\"].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bind_ngs[\"Experiments\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bind_ngs.loc[bind_ngs['aCBTX'] > 10000, 'Experiments'] = 'aCBTX_binder'\n",
    "bind_ngs.loc[bind_ngs['⍺-BuTx'] > 10000, 'Experiments'] = 'aBGTx_binder'\n",
    "bind_ngs = bind_ngs.loc[bind_ngs['Experiments'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bind_ngs[\"Experiments\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = bind_ngs[\"Experiments\"].tolist()\n",
    "full_sequences = bind_ngs[\"Full sequence\"].str.slice(0, 115).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\"\n",
    "name_data = \"aCBTX_max_data\"\n",
    "from transformers import RoFormerTokenizer, RoFormerModel\n",
    "if model_path == \"alchemab/antiberta2-cssp\":\n",
    "    tokenizer = RoFormerTokenizer.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "    model = RoFormerModel.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "    Analyse = AnalyseModel(\"alchemab/antiberta2-cssp\", model, tokenizer )\n",
    "else:\n",
    "    Analyse = AnalyseModel(model_path)\n",
    "reduced_X = Analyse.embed_cluster_label(full_sequences, labels, explained_variance_threshold = 0.9,\n",
    "                                        size_points = 15, n_neighbors = 15, min_dist = 0.1, alpha = 0.8, \n",
    "                                        cmap = \"tab10\", title = f\"Embedding of NGS sequences with identified binders against aBTX and aCBTX\")\n",
    "#reduced_X.to_csv(f\"{name_data}.csv\")\n",
    "\n",
    "Analyse.save_in_plots(f\"{name_data}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Measurements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def time_process(model_path, no_sequences = 1000, region_name = None):\n",
    "    start_time = time.time()\n",
    "    faqs_data = pd.read_csv(r\"C:\\Users\\nilsh\\my_projects\\ExpoSeq\\my_experiments\\max_new\\sequencing_report.csv\").head(no_sequences)\n",
    "    if region_name != None:\n",
    "        sequences = faqs_data[region_name].tolist()\n",
    "    else:\n",
    "        sequences = faqs_data[\"targetSequences\"].apply(translate_nucleotide_to_amino_acid).tolist()\n",
    "    from transformers import RoFormerTokenizer, RoFormerModel\n",
    "    if model_path == \"alchemab/antiberta2-cssp\":\n",
    "        tokenizer = RoFormerTokenizer.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "        model = RoFormerModel.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "        Analyse = AnalyseModel(\"alchemab/antiberta2-cssp\", model, tokenizer )\n",
    "    else:\n",
    "        Analyse = AnalyseModel(model_path)\n",
    "    X = Analyse.ModelSets._get_embeddings_parallel(sequences, )\n",
    "    end_time = time.time()\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    return elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_process(r\"facebook/esm2_t36_3B_UR50D\", 1000, region_name = None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = time_process(r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\", 1000, region_name = None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"alchemab/antiberta2-cssp\", r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\", r\"facebook/esm2_t6_8M_UR50D\", r\"facebook/esm2_t30_150M_UR50D\"]\n",
    "no_seq = 1000\n",
    "all_times = []\n",
    "region = None\n",
    "folds = 5\n",
    "for model in models:\n",
    "    fold_model = []\n",
    "    for fold in range(folds):\n",
    "        elapsed_time = time_process(model, no_seq, region_name = None )\n",
    "        fold_model.append(elapsed_time)\n",
    "    all_times.append(fold_model)\n",
    "\n",
    "means = [np.mean(sample_data) for sample_data in all_times]\n",
    "std_devs = [np.std(sample_data) for sample_data in all_times]\n",
    "\n",
    "names = [\"Antiberta2\", \"Self-trained model\", \"Esm-2b with 6 layers\", \"Esm-2b with 30 layers\"]\n",
    "assert len(names) == len(models), \"The number of models and names must be the same\"\n",
    "Analyse = AnalyseModel(r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\")\n",
    "Analyse.make_figure()\n",
    "Analyse.ax.bar(names, means, yerr = std_devs, capsize = 5, color = \"skyblue\", alpha = 0.7)\n",
    "Analyse.update_plot()\n",
    "Analyse.ax.set_ylabel(\"Time in seconds\", **Analyse.font_settings)\n",
    "Analyse.ax.set_xlabel(\"Model\",  **Analyse.font_settings)\n",
    "Analyse.ax.set_title(\"Computing time for embedding of 1000 sequences\", pad = 20, **Analyse.font_settings)\n",
    "Analyse.save_in_plots(\"time_comparison.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "means = [np.mean(sample_data) for sample_data in all_times]\n",
    "std_devs = [np.std(sample_data) for sample_data in all_times]\n",
    "print(np.mean([\n",
    "    125.92300152778625, 125.60665917396545, 124.9071409702301, 125.5185558795929]))\n",
    "print(np.std([125.92300152778625, 125.60665917396545, 124.9071409702301, 125.5185558795929]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\jupyter_scripts\\validate.ipynb Cell 61\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nilsh/my_projects/SeqLP/jupyter_scripts/validate.ipynb#Y115sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(all_seqs) \u001b[39m==\u001b[39m \u001b[39m10000\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nilsh/my_projects/SeqLP/jupyter_scripts/validate.ipynb#Y115sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nilsh/my_projects/SeqLP/jupyter_scripts/validate.ipynb#Y115sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m ppl \u001b[39m=\u001b[39m Analyse\u001b[39m.\u001b[39;49mcalculate_perplexity(all_seqs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nilsh/my_projects/SeqLP/jupyter_scripts/validate.ipynb#Y115sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(ppl)\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\src\\seqlp\\use_model.py:305\u001b[0m, in \u001b[0;36mAnalyseModel.calculate_perplexity\u001b[1;34m(self, sequences, pad_token_id)\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[39mdel\u001b[39;00m encodings[\u001b[39m\"\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    304\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 305\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mModelSets\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencodings, labels\u001b[39m=\u001b[39;49mlabels)\n\u001b[0;32m    306\u001b[0m metrics\u001b[39m=\u001b[39m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics(outputs\u001b[39m.\u001b[39mlogits, labels, pad_token_id)\n\u001b[0;32m    307\u001b[0m \u001b[39mreturn\u001b[39;00m metrics\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\transformers\\models\\esm\\modeling_esm.py:1008\u001b[0m, in \u001b[0;36mEsmForMaskedLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    999\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m \u001b[39m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1004\u001b[0m \u001b[39m    Used to hide legacy arguments that have been deprecated.\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1008\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mesm(\n\u001b[0;32m   1009\u001b[0m     input_ids,\n\u001b[0;32m   1010\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1011\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1012\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1013\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1014\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1015\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m   1016\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1017\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1018\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1019\u001b[0m )\n\u001b[0;32m   1020\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1021\u001b[0m prediction_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\transformers\\models\\esm\\modeling_esm.py:914\u001b[0m, in \u001b[0;36mEsmModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    905\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    907\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m    908\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    909\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    912\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    913\u001b[0m )\n\u001b[1;32m--> 914\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m    915\u001b[0m     embedding_output,\n\u001b[0;32m    916\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    917\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    918\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    919\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m    920\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    921\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    922\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    923\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    924\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    925\u001b[0m )\n\u001b[0;32m    926\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    927\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\transformers\\models\\esm\\modeling_esm.py:619\u001b[0m, in \u001b[0;36mEsmEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    608\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    609\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[0;32m    610\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    616\u001b[0m         output_attentions,\n\u001b[0;32m    617\u001b[0m     )\n\u001b[0;32m    618\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 619\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    620\u001b[0m         hidden_states,\n\u001b[0;32m    621\u001b[0m         attention_mask,\n\u001b[0;32m    622\u001b[0m         layer_head_mask,\n\u001b[0;32m    623\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    624\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    625\u001b[0m         past_key_value,\n\u001b[0;32m    626\u001b[0m         output_attentions,\n\u001b[0;32m    627\u001b[0m     )\n\u001b[0;32m    629\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\transformers\\models\\esm\\modeling_esm.py:509\u001b[0m, in \u001b[0;36mEsmLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    498\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    499\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    506\u001b[0m ):\n\u001b[0;32m    507\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    508\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 509\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    510\u001b[0m         hidden_states,\n\u001b[0;32m    511\u001b[0m         attention_mask,\n\u001b[0;32m    512\u001b[0m         head_mask,\n\u001b[0;32m    513\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    514\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    515\u001b[0m     )\n\u001b[0;32m    516\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    518\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\transformers\\models\\esm\\modeling_esm.py:443\u001b[0m, in \u001b[0;36mEsmAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    433\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    434\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    441\u001b[0m ):\n\u001b[0;32m    442\u001b[0m     hidden_states_ln \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states)\n\u001b[1;32m--> 443\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    444\u001b[0m         hidden_states_ln,\n\u001b[0;32m    445\u001b[0m         attention_mask,\n\u001b[0;32m    446\u001b[0m         head_mask,\n\u001b[0;32m    447\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    448\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    449\u001b[0m         past_key_value,\n\u001b[0;32m    450\u001b[0m         output_attentions,\n\u001b[0;32m    451\u001b[0m     )\n\u001b[0;32m    452\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    453\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\transformers\\models\\esm\\modeling_esm.py:380\u001b[0m, in \u001b[0;36mEsmSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    378\u001b[0m     attention_probs \u001b[39m=\u001b[39m attention_probs \u001b[39m*\u001b[39m head_mask\n\u001b[1;32m--> 380\u001b[0m context_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(attention_probs\u001b[39m.\u001b[39;49mto(value_layer\u001b[39m.\u001b[39;49mdtype), value_layer)\n\u001b[0;32m    382\u001b[0m context_layer \u001b[39m=\u001b[39m context_layer\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n\u001b[0;32m    383\u001b[0m new_context_layer_shape \u001b[39m=\u001b[39m context_layer\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_head_size,)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model_path = r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\"\n",
    "if model_path == \"alchemab/antiberta2-cssp\":\n",
    "    from transformers import RoFormerTokenizer, RoFormerForMaskedLM\n",
    "    tokenizer = RoFormerTokenizer.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "    model = RoFormerForMaskedLM.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "    Analyse = AnalyseModel(\"alchemab/antiberta2-cssp\", model, tokenizer )     \n",
    "else:\n",
    "    Analyse = AnalyseModel(model_path, load_masked_lm=True)\n",
    "sequences = pd.read_csv(r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\data\\training_data_LLM\\val.csv\", nrows = 10000)[\"sequence\"].tolist()\n",
    "all_seqs = []\n",
    "for seq in sequences:\n",
    "    if len(seq) < 240: # there are some sequence lengths in the data that dont make sense in the context of nanobodies. 240 because there are spaces in between each tokens\n",
    "        pass\n",
    "    else:\n",
    "        all_seqs.append(seq)\n",
    "    if len(all_seqs) == 10000:\n",
    "        break\n",
    "ppl = Analyse.calculate_perplexity(all_seqs)\n",
    "print(ppl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path =  r\"facebook/esm2_t6_8M_UR50D\"\n",
    "if model_path == \"alchemab/antiberta2-cssp\":\n",
    "    from transformers import RoFormerTokenizer, RoFormerForMaskedLM\n",
    "    tokenizer = RoFormerTokenizer.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "    model = RoFormerForMaskedLM.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "    Analyse = AnalyseModel(\"alchemab/antiberta2-cssp\", model, tokenizer )     \n",
    "else:\n",
    "    Analyse = AnalyseModel(model_path, load_masked_lm=True)\n",
    "sequences = pd.read_csv(r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\data\\training_data\\val.csv\", nrows = 1000)[\"sequence\"].tolist()\n",
    "all_seqs = []\n",
    "for seq in sequences:\n",
    "    if len(seq) < 240:\n",
    "        pass\n",
    "    else:\n",
    "        all_seqs.append(seq)\n",
    "    if len(all_seqs) == 1000:\n",
    "        break\n",
    "ppl = Analyse.calculate_perplexity(all_seqs)\n",
    "print(ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The model with lower perplexity might be heavily optimized towards predicting the next amino acid accurately, focusing narrowly on features that are directly predictive of the next outcome. This can lead to a situation where the embeddings, while effective for prediction, may not capture broader or more nuanced relationships between different types of amino acids.\n",
    "\n",
    " A reason for this can be that the model cannot distinguish between the cdr and other regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path =  r\"alchemab/antiberta2\"\n",
    "if model_path == \"alchemab/antiberta2\":\n",
    "    from transformers import RoFormerTokenizer, RoFormerForMaskedLM\n",
    "    tokenizer = RoFormerTokenizer.from_pretrained(\"alchemab/antiberta2\")\n",
    "    model = RoFormerForMaskedLM.from_pretrained(\"alchemab/antiberta2\")\n",
    "    Analyse = AnalyseModel(\"alchemab/antiberta2\", model, tokenizer )     \n",
    "else:\n",
    "    Analyse = AnalyseModel(model_path, load_masked_lm=True)\n",
    "sequences = pd.read_csv(r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\data\\training_data\\val.csv\", nrows = 1000)[\"sequence\"].tolist()\n",
    "all_seqs = []\n",
    "for seq in sequences:\n",
    "    if len(seq) < 240:\n",
    "        pass\n",
    "    else:\n",
    "        all_seqs.append(seq)\n",
    "    if len(all_seqs) == 1000:\n",
    "        break\n",
    "ppl = Analyse.calculate_perplexity(all_seqs, pad_token_id=0) # token id is 0, https://huggingface.co/alchemab/antiberta2/blob/main/vocab.txt\n",
    "print(ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path =  r\"facebook/esm2_t30_150M_UR50D\"\n",
    "if model_path == \"alchemab/antiberta2\":\n",
    "    from transformers import RoFormerTokenizer, RoFormerForMaskedLM\n",
    "    tokenizer = RoFormerTokenizer.from_pretrained(\"alchemab/antiberta2\")\n",
    "    model = RoFormerForMaskedLM.from_pretrained(\"alchemab/antiberta2\")\n",
    "    Analyse = AnalyseModel(\"alchemab/antiberta2\", model, tokenizer )     \n",
    "else:\n",
    "    Analyse = AnalyseModel(model_path, load_masked_lm=True)\n",
    "sequences = pd.read_csv(r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\data\\training_data\\val.csv\", nrows = 1000)[\"sequence\"].tolist()\n",
    "all_seqs = []\n",
    "for seq in sequences:\n",
    "    if len(seq) < 240:\n",
    "        pass\n",
    "    else:\n",
    "        all_seqs.append(seq)\n",
    "    if len(all_seqs) == 1000:\n",
    "        break\n",
    "ppl = Analyse.calculate_perplexity(all_seqs) # token id is 0, https://huggingface.co/alchemab/antiberta2/blob/main/vocab.txt\n",
    "print(ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check CDR3 positional importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nanobody_delphia.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "nanobody_delphia = pd.read_excel(r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\data\\nanobody_delphia_tidied.xlsx\")\n",
    "nanobody_delphia = nanobody_delphia.dropna(subset=['CDR3', 'Full sequence'])\n",
    "\n",
    "\n",
    "def pad_sequence(subseq, fullseq):\n",
    "    start_index = fullseq.find(subseq)\n",
    "    if start_index == -1:\n",
    "        # If subsequence not found, return padded version of the full sequence\n",
    "        return \"\"\n",
    "    \n",
    "    # Create a list of <PAD> for each character in full sequence\n",
    "    padded_sequence = ['<pad>' for _ in fullseq]\n",
    "    \n",
    "    # Replace the <PAD> tokens with the subsequence at the correct position\n",
    "    for i in range(len(subseq)):\n",
    "        padded_sequence[start_index + i] = subseq[i]\n",
    "    \n",
    "    # Join the sequence with space\n",
    "    return ' '.join(padded_sequence)\n",
    "\n",
    "# Apply the function to each row in DataFrame\n",
    "sequencing_report = pd.read_csv(r\"C:\\Users\\nilsh\\my_projects\\ExpoSeq\\my_experiments\\max_new\\sequencing_report.csv\",)\n",
    "mask = ~sequencing_report['targetSequences'].str.contains('region_not_covered')\n",
    "\n",
    "# Apply the mask to the DataFrame to keep only the rows where the condition is True\n",
    "sequencing_report = sequencing_report[mask]\n",
    "sequencing_report[\"full_seq\"] = sequencing_report[\"targetSequences\"].apply(translate_nucleotide_to_amino_acid)\n",
    "sequencing_report[\"padded_sequence\"] = sequencing_report.apply(lambda x: pad_sequence(x['aaSeqCDR3'], x['full_seq']), axis=1)\n",
    "mask = sequencing_report['padded_sequence'].str.len() > 0\n",
    "report = sequencing_report[mask]\n",
    "\n",
    "#nanobody_delphia['PaddedSequence'] = nanobody_delphia.apply(lambda x: pad_sequence(x['CDR3'], x['Full sequence']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencing_report[\"padded_sequence\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = report.groupby(\"Experiment\").head(200)\n",
    "padded_sequences = report[\"padded_sequence\"].tolist()\n",
    "labels = report[\"Experiment\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpadded_sequences = report[\"aaSeqCDR3\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picture_path = \"unpadded_cdr3_faqs_data_25n\"\n",
    "title_extension = \"specifically trained model\"\n",
    "model_path = r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\"\n",
    "Analyse = AnalyseModel(model_path)\n",
    "reduced_X = Analyse.embed_cluster_label(unpadded_sequences, labels, explained_variance_threshold = 0.9, n_neighbors = 25, min_dist = 0.1, alpha = 0.8, cmap = \"Set2\", title = f\"Unpadded CDR3 sequences with n = 25\", color_list = color_list)\n",
    "reduced_X.to_csv(f\"{picture_path}.csv\")\n",
    "Analyse.save_in_plots(f\"{picture_path}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nanobody_delphia.shape[0])\n",
    "nanobody_delphia[\"Ag1_modified\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequences = nanobody_delphia[\"PaddedSequence\"].tolist()\n",
    "labels = nanobody_delphia[\"Ag1_modified\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picture_path = \"position_cdr_importance\"\n",
    "title_extension = \"specifically trained model\"\n",
    "model_path = r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\"\n",
    "Analyse = AnalyseModel(model_path)\n",
    "reduced_X = Analyse.embed_cluster_label(padded_sequences, labels, explained_variance_threshold = 0.9, n_neighbors = 15, min_dist = 0.1, alpha = 0.8, cmap = \"Set2\", title = f\"Embedding of position specific padded CDR3 sequences.\")\n",
    "reduced_X.to_csv(f\"{picture_path}.csv\")\n",
    "\n",
    "Analyse.save_in_plots(f\"{picture_path}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analyse = AnalyseModel(r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\", load_masked_lm=True)\n",
    "ppl = Analyse.calculate_perplexity(padded_sequences)\n",
    "print(ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picture_path = \"position_cdr_dumped\"\n",
    "title_extension = \"specifically trained model\"\n",
    "unpadded_sequences = nanobody_delphia[\"CDR3\"].tolist()\n",
    "model_path = r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\"\n",
    "Analyse = AnalyseModel(model_path)\n",
    "reduced_X = Analyse.embed_cluster_label(unpadded_sequences, labels, explained_variance_threshold = 0.9, n_neighbors = 15, min_dist = 0.1, alpha = 0.8, cmap = \"Set2\", title = f\"Embedding of unpadded CDR3 sequences with {title_extension}\")\n",
    "reduced_X.to_csv(f\"{picture_path}.csv\")\n",
    "\n",
    "Analyse.save_in_plots(f\"{picture_path}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpadded_sequences = faqs_data[\"aaSeqCDR3\"].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analyse = AnalyseModel(r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\", load_masked_lm=True)\n",
    "ppl = Analyse.calculate_perplexity(unpadded_sequences)\n",
    "print(ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All complementary regions but no framework regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "nanobody_delphia = pd.read_excel(r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\data\\nanobody_delphia_tidied.xlsx\")\n",
    "nanobody_delphia = nanobody_delphia.dropna(subset=['CDR1', 'CDR2', 'CDR3', 'Full sequence'])  \n",
    "\n",
    "def pad_multiple_sequences(fullseq, *subseqs):\n",
    "    # Create a list of <PAD> for each character in full sequence\n",
    "    padded_sequence = ['<pad>' for _ in range(len(fullseq))]\n",
    "    \n",
    "    # Process each subsequence\n",
    "    for subseq in subseqs:\n",
    "        start_index = fullseq.find(subseq)\n",
    "        if start_index != -1:\n",
    "            # Replace the <PAD> tokens with the subsequence at the correct position\n",
    "            for i in range(len(subseq)):\n",
    "                padded_sequence[start_index + i] = subseq[i]\n",
    "\n",
    "    # Join the sequence with space\n",
    "    return ' '.join(padded_sequence)\n",
    "\n",
    "# Apply the function to each row in DataFrame to include CDR1, CDR2, CDR3\n",
    "nanobody_delphia['PaddedSequence'] = nanobody_delphia.apply(lambda x: pad_multiple_sequences(x['Full sequence'], x['Framework 1'], x['Framework 2'], x['Framework 3']), axis=1)\n",
    "\n",
    "# Example of saving or viewing the results\n",
    "print(nanobody_delphia[['Full sequence', 'PaddedSequence']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequences = nanobody_delphia[\"PaddedSequence\"].tolist()\n",
    "labels = nanobody_delphia[\"Ag1_modified\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picture_path = \"position_all_fr_importance\"\n",
    "title_extension = \"specifically trained model\"\n",
    "model_path = r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\"\n",
    "Analyse = AnalyseModel(model_path)\n",
    "reduced_X = Analyse.embed_cluster_label(padded_sequences, labels, explained_variance_threshold = 0.9, n_neighbors = 15, min_dist = 0.1, alpha = 0.8, cmap = \"Set2\", title = f\"Embedding of position specific padded FR1, FR2 and FR3 sequences.\")\n",
    "reduced_X.to_csv(f\"{picture_path}.csv\")\n",
    "\n",
    "Analyse.save_in_plots(f\"{picture_path}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make barplot with venom composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venom_comp = pd.read_excel(r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\data\\venom_compmosition.xlsx\", index_col = 0)\n",
    "venom_comp.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_targets = venom_comp.loc[[\"Dp4\", \"Nu6\", \"Nm8\",  \"Dp8\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "legend_settings = {'loc': 'center left','facecolor': 'black',  'bbox_to_anchor': (1, 0.5), 'ncols': 1, 'fontsize': 16, 'frameon': True, 'framealpha': 1, 'facecolor': 'white', 'mode': None, 'title_fontsize': 'large', 'title_fontsize': 'large'}\n",
    "# Creating the stacked bar plot\n",
    "Analyse.make_figure()\n",
    "\n",
    "ax = specific_targets.plot(kind='bar', stacked=True, figsize=(10, 7), colormap= \"tab10\", alpha = 0.9, ax = Analyse.ax)\n",
    "label_settings = {'fontfamily': 'serif',\n",
    " 'fontsize': '14',\n",
    " 'fontstyle': 'normal',\n",
    " }\n",
    "# Adding labels and title\n",
    "plt.xlabel('Venom', **Analyse.font_settings_normal)\n",
    "plt.ylabel('Composition (%)', **Analyse.font_settings_normal)\n",
    "plt.title('Toxin Composition by Venom Type', **Analyse.font_settings_title)\n",
    "plt.xticks(rotation=0)\n",
    "# Show legend and plot\n",
    "plt.legend(title='Toxin Types', **legend_settings)\n",
    "Analyse.update_plot()\n",
    "Analyse.save_in_plots(\"venom_composition.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analyse.font_settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating with machine learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencing_report[\"Experiment\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Data = DataPipeline(no_sequences = 1000000, model = r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\models\\mini_all_t4_12h\\results\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_encoded = [0 if item == \"cLNTX_non-bind\" else 1 for item in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Data.init_sequencing_report['Experiment'].tolist()\n",
    "ML = SupervisedML(Data.X, y_encoded, cv_components = 5)\n",
    "model = ML.logistic_regression()\n",
    "scores = ML.do_scikits_cv(model)\n",
    "ML.do_nn_cv(num_epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = DataPipeline(no_sequences = 1000000, model = esm_small)\n",
    "y = Data.init_sequencing_report['Experiment'].tolist()\n",
    "y_encoded = [0 if item == \"cLNTX_non-bind\" else 1 for item in y]\n",
    "ML = SupervisedML(Data.X, y_encoded, cv_components = 5)\n",
    "model = ML.logistic_regression()\n",
    "scores = ML.do_scikits_cv(model)\n",
    "ML.do_nn_cv(num_epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esm_accuracies = [0.7718, 0.7768, 0.7859, 0.7975, 0.7714] # 0.9562920928001404\n",
    "trained_model_accuracies = [0.8075, 0.8174,0.8178, 0.7946, 0.8141]\n",
    "AnalyseModel.paired_t_test(esm_accuracies, trained_model_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show an epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analyse = AnalyseModel(r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\")\n",
    "\n",
    "Analyse.make_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ax = Analyse.ax\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extracted accuracy values from the provided data for FOLD 1 and FOLD 2\n",
    "accuracies_fold1 = [0.7556, 0.7714, 0.7797, 0.7917, 0.8046, 0.8112, 0.8137, 0.8141, 0.8154, 0.8174]\n",
    "accuracies_fold2 = [0.7373, 0.7498, 0.7589, 0.7589, 0.7651, 0.7755, 0.7788, 0.7826, 0.7838, 0.7859]\n",
    "\n",
    "# Creating an index for the epochs\n",
    "epochs = range(1, 11)\n",
    "\n",
    "# Create a figure and an ax object\n",
    "\n",
    "# Plot the data on the ax object\n",
    "ax.plot(epochs, accuracies_fold1, label='ESM-t6 8M UR50D')\n",
    "ax.plot(epochs, accuracies_fold2, label='Self trained model')\n",
    "\n",
    "# Adding labels and title\n",
    "ax.set_xlabel('Epoch', Analyse.font_settings_normal)\n",
    "ax.set_ylabel('Accuracy', Analyse.font_settings_normal)\n",
    "ax.set_title('Exemplified Validation Accuracies Over Epochs for the models', Analyse.font_settings_title)\n",
    "ax.legend()\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.array(esm_accuracies))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "three class problem: cLNTX ++ and aBGTX-+ | cLNTX +- aBGTX +- | Non bing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_columns = [\"cLNTX_non-bind\", \"cLNTX_++\", \"aBGTX_-+\", \"cLNTX_+-\", \"aBGTX_+-\"]\n",
    "model_big = r\"c:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\models\\t12_320_lastlayer_2106_seqs\"\n",
    "Data = DataPipeline(no_sequences = 1000000, model = model_big, choose_labels= chosen_columns)\n",
    "y = Data.init_sequencing_report['Experiment'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_encoded = [0 if item == \"cLNTX_non-bind\" else 1 if item in ['cLNTX_++', 'aBGTX_-+'] else 2 if item in ['cLNTX_+-', 'aBGTX_+-'] else item for item in y]\n",
    "\n",
    "ML = SupervisedML(Data.X, y_encoded, cv_components = 5)\n",
    "model = ML.logistic_regression()\n",
    "scores = ML.do_scikits_cv(model)\n",
    "ML.do_nn_cv(num_epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_columns = [\"cLNTX_non-bind\", \"cLNTX_++\", \"aBGTX_-+\", \"cLNTX_+-\", \"aBGTX_+-\"]\n",
    "Data = DataPipeline(no_sequences = 1000000, model = r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\models\\mini_all_t4_12h\\results\", choose_labels= chosen_columns)\n",
    "y = Data.init_sequencing_report['Experiment'].tolist()\n",
    "y_encoded = [0 if item == \"cLNTX_non-bind\" else 1 if item in ['cLNTX_++', 'aBGTX_-+'] else 2 if item in ['cLNTX_+-', 'aBGTX_+-'] else item for item in y]\n",
    "\n",
    "ML = SupervisedML(Data.X, y_encoded, cv_components = 5)\n",
    "model = ML.logistic_regression()\n",
    "scores = ML.do_scikits_cv(model)\n",
    "ML.do_nn_cv(num_epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esm_accuracies = [0.7371, 0.7128, 0.7041, 0.6622]\n",
    "trained_model_accuracies = [0.7012, 0.7061,0.7182, 0.7693, 0.7350]\n",
    "AnalyseModel.paired_t_test(esm_accuracies, trained_model_accuracies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
