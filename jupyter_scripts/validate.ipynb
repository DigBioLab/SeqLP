{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and important functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: c:\\Users\\nilsh\\my_projects\\SeqLP\\jupyter_scripts\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "dir_project = os.path.dirname(os.getcwd())\n",
    "path = os.path.join(dir_project, 'src')\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path)\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "#os.chdir(os.path.join(dir_project, 'src'))\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from seqlp.visualize.supervised_ml import DataPipeline, SupervisedML\n",
    "import glob\n",
    "from seqlp.use_model import AnalyseModel\n",
    "from Bio import SeqIO\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nanobody_delphia_no_ag_filter = pd.read_excel(r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\data\\nanobody_delphia_no_ag_filter.xlsx\")\n",
    "nanobody_delphia_ag_filter = pd.read_excel(r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\data\\nanobody_delphia_tidied.xlsx\")\n",
    "binding_data = pd.read_excel(r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\data\\all_binding_data.xlsx\")\n",
    "sequencing_report = pd.read_csv(r\"C:\\Users\\nilsh\\my_projects\\ExpoSeq\\my_experiments\\max_new\\sequencing_report.csv\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nanobody_model = r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\"\n",
    "esm_small = r\"facebook/esm2_t6_8M_UR50D\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_fasta(fasta_file):\n",
    "    sequences = []\n",
    "    headers = []\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        sequences.append(str(record.seq))\n",
    "        headers.append(record.id)\n",
    "    return sequences, headers\n",
    "\n",
    "from Bio.Seq import Seq\n",
    "def translate_nucleotide_to_amino_acid(nucleotide_sequence):\n",
    "    seq = Seq(nucleotide_sequence)\n",
    "    return str(seq.translate())    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = DataPipeline(model = r\"C:\\Users\\nilsh\\my_projects\\ExpoSeq\\models\\nanobody_full\",\n",
    "             path_seq_report = r\"C:\\Users\\nilsh\\my_projects\\ExpoSeq\\my_experiments\\max_new\\sequencing_report.csv\",\n",
    "             no_sequences = 100000,  # take basically all sequences\n",
    "             pca_components=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencing_report = Data.init_sequencing_report\n",
    "sequencing_report[\"v_gene\"] = sequencing_report['allVHitsWithScore'].str.split('*').str[0]\n",
    "experiments = sequencing_report[\"Experiment\"].unique().tolist()\n",
    "v_family = sequencing_report[\"v_gene\"].tolist()\n",
    "sequencing_report[\"full_seq\"] = Data.full_sequences\n",
    "print(f\"No. of sequences in report: {sequencing_report.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(r'c:\\Users\\nilsh\\OneDrive\\Desktop\\master_thesis\\train_model\\concatenatednanobody_full_train.csv.gz', compression='gzip')\n",
    "sequences = training_data.iloc[:, 0]\n",
    "sequences = sequences.str.replace(' ', '')\n",
    "print(f\"No. of training sequences: {training_data.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = sequencing_report['full_seq'].isin(sequences)\n",
    "\n",
    "# Step 4: Filter the DataFrame\n",
    "sequencing_report = sequencing_report[~mask]\n",
    "print(f\"No. of sequences in report after filtering training sequences: {sequencing_report.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity Analysis\n",
    "\n",
    "The goal is to find a metric which captures how decisive or confused the model is. We will use perplexity for this which is 2 ** Entropy.\n",
    "The highest entropy for this task is 20. That means that each amino acid per position can appear with equal probability which is very bad.\n",
    "\n",
    "Cons of this metric:\n",
    "- not good for final evaluation, since it just measures the model's confidence not its accuracy\n",
    "- A model with a lower perplexity can still be worse because it can just be very decisive but then in the clsutering would kinda fail to represent the sequences meaningfully.\n",
    "\n",
    "-> your model does not output logits, so it does not really make sense to do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_perplexity(perplexities):\n",
    "    mean_perplexity = np.mean(perplexities)\n",
    "    median_perplexity = np.median(perplexities)\n",
    "    std_dev = np.std(perplexities)\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(perplexities, bins=30, alpha=0.7, color='blue', label='Perplexity')\n",
    "    plt.axvline(mean_perplexity, color='r', linestyle='dashed', linewidth=1, label=f'Mean: {mean_perplexity:.2f}')\n",
    "    plt.axvline(median_perplexity, color='g', linestyle='dashed', linewidth=1, label=f'Median: {median_perplexity:.2f}')\n",
    "    plt.title('Distribution of Perplexity Scores')\n",
    "    plt.xlabel('Perplexity')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqlp.visualize.load_model import LoadModel\n",
    "\n",
    "NanobodyModel = LoadModel(r\"C:\\Users\\nilsh\\my_projects\\ExpoSeq\\models\\nanobody_full\")\n",
    "sequences = sequencing_report[\"full_seq\"].tolist()\n",
    "perplexities = []\n",
    "for sequence in sequences:\n",
    "    perplexity = NanobodyModel._get_perplexity(sequence)\n",
    "    perplexities.append(perplexity)\n",
    "perplexities = np.array(perplexities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COmpare model architectures based on no of components to reach 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = pd.read_csv(r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\data\\training_data\\val.csv\", nrows = 100000)[\"sequence\"].tolist()\n",
    "Analyse = AnalyseModel(r\"c:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\models\\t6_320_lastlayer_2106_seqs\")\n",
    "n_comp_collected = []\n",
    "sequence_lengths = [100, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "\n",
    "for length in sequence_lengths:\n",
    "    chunk = sequences[:length]\n",
    "    n_comp_collected.append(Analyse.no_components_for_sequences(chunk))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for clustering\n",
    "\n",
    "- Show how model clusters nanobodies with binding data. This model should do that gradually while the others should have problems with that\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse different layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_sequences, experiments = DataPipeline.wrangle_report(sequencing_report)\n",
    "import numpy as np\n",
    "full_sequences = np.array(full_sequences).reshape(-1, 12050).T\n",
    "experiments = np.array(experiments).reshape(-1, 12050).T\n",
    "\n",
    "full_sequences.shape\n",
    "\n",
    "# Create a DataFrame from the arrays\n",
    "df = pd.DataFrame({\n",
    "    \"CDR3\": sequencing_report[\"aaSeqCDR3\"],\n",
    "    'Full Sequences': full_sequences.flatten(),  # Flattening in case the array is 2D but should be 1D per column\n",
    "    'Experiments': experiments.flatten(),\n",
    "\n",
    "})\n",
    "df_filtered = df.drop_duplicates(subset = [\"CDR3\"], keep = \"last\")\n",
    "# This creates a mask that is True for rows where 'Full Sequences' does not contain 'region_not_covered'\n",
    "mask = ~df['Full Sequences'].str.contains('region_not_covered')\n",
    "\n",
    "# Apply the mask to the DataFrame to keep only the rows where the condition is True\n",
    "df_filtered = df[mask]\n",
    "\n",
    "df_filtered = df_filtered.groupby(\"Experiments\").head(200)\n",
    "df_filtered[\"Experiments\"].unique()\n",
    "experiments = df_filtered[\"Experiments\"].tolist()\n",
    "full_sequences = df_filtered[\"Full Sequences\"].tolist()\n",
    "cdr3 = df_filtered[\"CDR3\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('Set2')\n",
    "\n",
    "# Number of colors in Set2\n",
    "n_colors = cmap.N\n",
    "\n",
    "# Retrieve each color from the colormap\n",
    "colors = [cmap(i / float(n_colors - 1)) for i in range(n_colors)]\n",
    "color_list = [colors[0], colors[7], colors[1], colors[2], colors[3], colors[4], colors[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = [colors[0], colors[7], colors[1], colors[2], colors[3], colors[6], colors[5]]\n",
    "def sequence_label(model_path, picture_path, title_extension, sequences, labels,):\n",
    "    from transformers import RoFormerTokenizer, RoFormerModel\n",
    "    if model_path == \"alchemab/antiberta2-cssp\":\n",
    "        tokenizer = RoFormerTokenizer.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "        model = RoFormerModel.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "        Analyse = AnalyseModel(\"alchemab/antiberta2-cssp\", model, tokenizer )\n",
    "    else:\n",
    "        Analyse = AnalyseModel(model_path)\n",
    "    reduced_X = Analyse.embed_cluster_label(sequences, labels, explained_variance_threshold = 0.9,\n",
    "                                            size_points = 15, n_neighbors = 15, min_dist = 0.1, alpha = 0.8, \n",
    "                                            cmap = \"Set2\", title = title_extension, color_list = color_list)\n",
    "    reduced_X.to_csv(f\"{picture_path}.csv\")\n",
    "\n",
    "    Analyse.save_in_plots(f\"{picture_path}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = nanobody_delphia_ag_filter[\"Full sequence\"].tolist()\n",
    "labels = nanobody_delphia_ag_filter[\"Ag1_modified\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_label(r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\models\\t12_320_lastlayer_2106_seqs\", r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\validation_embedding\\measure_pca_little\", \n",
    "               \"6 attention layers and 320 nodes in last hidden layer\", sequences, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare binding data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed specific targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sequence_label(model_path, picture_path, title_extension):\n",
    "    signals = nanobody_delphia_ag_filter[\"Ag1_Raw signals\"].tolist()\n",
    "    labels = nanobody_delphia_ag_filter[\"Ag1_modified\"].tolist()\n",
    "    sequences = nanobody_delphia_ag_filter[\"Full sequence\"].tolist()\n",
    "    from transformers import RoFormerTokenizer, RoFormerModel\n",
    "    if model_path == \"alchemab/antiberta2-cssp\":\n",
    "        tokenizer = RoFormerTokenizer.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "        model = RoFormerModel.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "        Analyse = AnalyseModel(\"alchemab/antiberta2-cssp\", model, tokenizer )\n",
    "    else:\n",
    "        Analyse = AnalyseModel(model_path)\n",
    "    reduced_X = Analyse.embed_cluster_label(sequences, labels, explained_variance_threshold = 0.9,size_points = 50, n_neighbors = 15, min_dist = 0.1, alpha = 0.8, cmap = \"Set2\", title = f\"Embedding of nanobody sequences with {title_extension}\")\n",
    "    reduced_X[\"Name VHH\"] = nanobody_delphia_ag_filter[\"Name VHH\"]\n",
    "    reduced_X.to_csv(f\"{picture_path}.csv\")\n",
    "\n",
    "    Analyse.save_in_plots(f\"{picture_path}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_label(r\"facebook/esm2_t6_8M_UR50D\", r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\validation_embedding\\embedding_esm_t6_90%\", \"Esm-2b with 6 layers as baseline model\", )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_label(r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\", r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\validation_embedding\\embedding_self_trained_90%\", \"trained model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_label(r\"alchemab/antiberta2-cssp\", r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\validation_embedding\\embedding_antiberta_90%\", r\"Antiberta2\")\n",
    "sequence_label(r\"facebook/esm2_t6_8M_UR50D\", r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\validation_embedding\\embedding_esm_t6_90%\", \"Esm-2b with 6 layers as baseline model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_label(r\"facebook/esm2_t36_3B_UR50D\", r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\validation_embedding\\embedding_esm_t36_90%\", \"Esm-2b with 36 layers as baseline model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just embed whole venoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nanobody_delphia = nanobody_delphia_no_ag_filter.dropna(subset=['Ag1_modified'])\n",
    "print(nanobody_delphia[\"Ag1_modified\"].unique())\n",
    "print(nanobody_delphia.shape)\n",
    "nanobody_delphia = nanobody_delphia[nanobody_delphia[\"Ag1_modified\"] != \"SVSP\"]\n",
    "nanobody_delphia = nanobody_delphia[nanobody_delphia[\"Ag1_modified\"] != \"Dv4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('Set2')\n",
    "cmap_2 = plt.get_cmap('Set1')\n",
    "# Number of colors in Set2\n",
    "n_colors = cmap.N\n",
    "n_colors_2 = cmap_2.N\n",
    "# Retrieve each color from the colormap\n",
    "colors_2 = [cmap_2(i / float(n_colors_2 - 1)) for i in range(n_colors_2)]\n",
    "\n",
    "colors = [cmap(i / float(n_colors - 1)) for i in range(n_colors)]\n",
    "color_list = [colors_2[0], colors[5], colors_2[1], colors[2], colors[1],  colors[4], colors_2[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\"\n",
    "picture_path = \"all_venoms_self_trained_90%\"\n",
    "title_extension = \"Embedding of \"\n",
    "signals = nanobody_delphia[\"Ag1_Raw signals\"].tolist()\n",
    "labels = nanobody_delphia[\"Ag1_modified\"].tolist()\n",
    "sequences = nanobody_delphia[\"Full sequence\"].tolist()\n",
    "from transformers import RoFormerTokenizer, RoFormerModel\n",
    "if model_path == \"alchemab/antiberta2-cssp\":\n",
    "    tokenizer = RoFormerTokenizer.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "    model = RoFormerModel.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "    Analyse = AnalyseModel(\"alchemab/antiberta2-cssp\", model, tokenizer )\n",
    "else:\n",
    "    Analyse = AnalyseModel(model_path)\n",
    "reduced_X = Analyse.embed_cluster_label(sequences, labels, explained_variance_threshold = 0.9,size_points = 35, n_neighbors = 15, min_dist = 0.1, alpha = 0.9, cmap = \"Set2\", title = f\"Embedding of nanobody sequences selected against different venoms or toxins\", color_list = color_list)\n",
    "reduced_X.to_csv(f\"{picture_path}.csv\")\n",
    "\n",
    "Analyse.save_in_plots(f\"{picture_path}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('Set2')\n",
    "\n",
    "# Number of colors in Set2\n",
    "n_colors = cmap.N\n",
    "\n",
    "# Retrieve each color from the colormap\n",
    "colors = [cmap(i / float(n_colors - 1)) for i in range(n_colors)]\n",
    "color_list = [colors[0], colors[7], colors[1], colors[2], colors[3], colors[4], colors[5]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencing_report = pd.read_csv(r\"C:\\Users\\nilsh\\my_projects\\ExpoSeq\\my_experiments\\max_new\\sequencing_report.csv\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the arrays\n",
    "df = pd.DataFrame({\n",
    "    \"CDR3\": sequencing_report[\"aaSeqCDR3\"],\n",
    "    'Full Sequences': full_sequences.flatten(),  # Flattening in case the array is 2D but should be 1D per column\n",
    "    'Experiments': experiments.flatten(),\n",
    "\n",
    "})\n",
    "df_filtered = df.drop_duplicates(subset = [\"CDR3\"], keep = \"last\")\n",
    "# This creates a mask that is True for rows where 'Full Sequences' does not contain 'region_not_covered'\n",
    "mask = ~df['Full Sequences'].str.contains('region_not_covered')\n",
    "\n",
    "# Apply the mask to the DataFrame to keep only the rows where the condition is True\n",
    "df_filtered = df[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.groupby(\"Experiments\").head(200)\n",
    "df_filtered[\"Experiments\"].unique()\n",
    "experiments = df_filtered[\"Experiments\"].tolist()\n",
    "full_sequences = df_filtered[\"Full Sequences\"].tolist()\n",
    "cdr3 = df_filtered[\"CDR3\"].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "color_list = [colors[0], colors[7], colors[1], colors[2], colors[3], colors[6], colors[5]]\n",
    "def sequence_label(model_path, picture_path, title_extension, sequences, labels,):\n",
    "    from transformers import RoFormerTokenizer, RoFormerModel\n",
    "    if model_path == \"alchemab/antiberta2-cssp\":\n",
    "        tokenizer = RoFormerTokenizer.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "        model = RoFormerModel.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "        Analyse = AnalyseModel(\"alchemab/antiberta2-cssp\", model, tokenizer )\n",
    "    else:\n",
    "        Analyse = AnalyseModel(model_path)\n",
    "    reduced_X = Analyse.embed_cluster_label(sequences, labels, explained_variance_threshold = 0.9,\n",
    "                                            size_points = 15, n_neighbors = 15, min_dist = 0.1, alpha = 0.8, \n",
    "                                            cmap = \"Set3\", title = f\"Embedding of CDR3 sequences with {title_extension}\", color_list = color_list)\n",
    "    reduced_X.to_csv(f\"{picture_path}.csv\")\n",
    "\n",
    "    Analyse.save_in_plots(f\"{picture_path}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sequence_label(r\"facebook/esm2_t6_8M_UR50D\", r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\validation_embedding\\embedding_baseline_max_data_cdr3\", \n",
    "               \"Esm-2b with 6 layers as baseline model\", cdr3, experiments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_label(r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\", r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\validation_embedding\\embedding_nanobody_model_max_data_cdr3\", \n",
    "               \"trained model\", cdr3, experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Seq import Seq\n",
    "def translate_nucleotide_to_amino_acid(nucleotide_sequence):\n",
    "    seq = Seq(nucleotide_sequence)\n",
    "    return str(seq.translate())    \n",
    "\n",
    "\n",
    "model_path = r\"alchemab/antiberta2-cssp\"\n",
    "name_data = \"embedding_antiberta_non_binding\"\n",
    "nanobody_delphia = pd.read_excel(r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\jupyter_scripts\\antibody_tidied.xlsx\")\n",
    "consensus_binding = nanobody_delphia[nanobody_delphia[\"Ag1\"] == \"scNTX\"]\n",
    "sequences = consensus_binding[\"Full sequence\"].tolist()\n",
    "signals = consensus_binding[\"Ag1_Raw signals\"].tolist()\n",
    "faqs_data = pd.read_csv(r\"C:\\Users\\nilsh\\my_projects\\ExpoSeq\\my_experiments\\max_new\\sequencing_report.csv\")\n",
    "faqs_data = faqs_data.groupby(\"Experiment\").head(200)\n",
    "binder_unlabeled = faqs_data[faqs_data[\"Experiment\"] == \"cLNTX_bind\"]\n",
    "non_binder = faqs_data[faqs_data[\"Experiment\"] == \"cLNTX_non-bind\"]\n",
    "seq_binder = non_binder[\"targetSequences\"].apply(translate_nucleotide_to_amino_acid).tolist()\n",
    "seq_non_binder = non_binder[\"targetSequences\"].apply(translate_nucleotide_to_amino_acid).tolist()\n",
    "binding_values_non_binder = len(seq_non_binder) * [0]\n",
    "binding_values_binder = len(seq_binder) * [0]\n",
    "all_binding =  binding_values_binder +binding_values_non_binder + signals\n",
    "all_sequences =  seq_binder + seq_non_binder + sequences\n",
    "labels =  len(seq_binder) * [\"cLNTX binder\"] + len(binding_values_non_binder) * [\"Non Binder\"] + len(signals) * [\"cLNTX binder\"]\n",
    "\n",
    "from transformers import RoFormerTokenizer, RoFormerModel\n",
    "if model_path == \"alchemab/antiberta2-cssp\":\n",
    "    tokenizer = RoFormerTokenizer.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "    model = RoFormerModel.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "    Analyse = AnalyseModel(\"alchemab/antiberta2-cssp\", model, tokenizer )\n",
    "else:\n",
    "    Analyse = AnalyseModel(model_path)\n",
    "reduced_X = Analyse.embed_cluster_sequences(all_sequences, labels, all_binding, explained_variance_threshold = 0.9, n_neighbors = 15, min_dist = 0.1, alpha = 0.8, cmap = \"inferno\", title = \"Embedding of nanobody sequences with Antiberta 2.\")\n",
    "#reduced_X.to_csv(f\"{name_data}.csv\")\n",
    "\n",
    "Analyse.save_in_plots(f\"{name_data}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate with binding data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sequences, experiments = DataPipeline.wrangle_report(sequencing_report)\n",
    "import numpy as np\n",
    "full_sequences = np.array(full_sequences).reshape(-1, 12050).T\n",
    "experiments = np.array(experiments).reshape(-1, 12050).T\n",
    "\n",
    "full_sequences.shape\n",
    "df = pd.DataFrame({\n",
    "    \"CDR3\": sequencing_report[\"aaSeqCDR3\"],\n",
    "    'Full sequence': full_sequences.flatten(),  # Flattening in case the array is 2D but should be 1D per column\n",
    "    'Experiments': experiments.flatten(),\n",
    "\n",
    "})\n",
    "df_filtered = df.drop_duplicates(subset = [\"CDR3\"], keep = \"last\")\n",
    "# This creates a mask that is True for rows where 'Full Sequences' does not contain 'region_not_covered'\n",
    "mask = ~df['Full sequence'].str.contains('region_not_covered')\n",
    "\n",
    "# Apply the mask to the DataFrame to keep only the rows where the condition is True\n",
    "df_filtered = df[mask]\n",
    "df_filtered = df_filtered.groupby(\"Experiments\").head(200)\n",
    "bind_ngs = df_filtered.merge(binding_data, on = \"Full sequence\", how = \"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bind_ngs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_sequences = bind_ngs[\"Full sequence\"].tolist()\n",
    "bind_ngs = bind_ngs.fillna(0)\n",
    "signals = bind_ngs[[\"⍺-BuTx\", \"aCBTX\"]]\n",
    "aCBTX = bind_ngs[\"aCBTX\"].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bind_ngs[\"Experiments\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bind_ngs.loc[bind_ngs['aCBTX'] > 10000, 'Experiments'] = 'aCBTX_binder'\n",
    "bind_ngs.loc[bind_ngs['⍺-BuTx'] > 10000, 'Experiments'] = 'aBGTx_binder'\n",
    "bind_ngs = bind_ngs.loc[bind_ngs['Experiments'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bind_ngs[\"Experiments\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = bind_ngs[\"Experiments\"].tolist()\n",
    "full_sequences = bind_ngs[\"Full sequence\"].str.slice(0, 115).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\"\n",
    "name_data = \"aCBTX_max_data\"\n",
    "from transformers import RoFormerTokenizer, RoFormerModel\n",
    "if model_path == \"alchemab/antiberta2-cssp\":\n",
    "    tokenizer = RoFormerTokenizer.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "    model = RoFormerModel.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "    Analyse = AnalyseModel(\"alchemab/antiberta2-cssp\", model, tokenizer )\n",
    "else:\n",
    "    Analyse = AnalyseModel(model_path)\n",
    "reduced_X = Analyse.embed_cluster_label(full_sequences, labels, explained_variance_threshold = 0.9,\n",
    "                                        size_points = 15, n_neighbors = 15, min_dist = 0.1, alpha = 0.8, \n",
    "                                        cmap = \"tab10\", title = f\"Embedding of NGS sequences with identified binders against aBTX and aCBTX\")\n",
    "#reduced_X.to_csv(f\"{name_data}.csv\")\n",
    "\n",
    "Analyse.save_in_plots(f\"{name_data}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Measurements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def time_process(model_path, no_sequences = 1000, region_name = None):\n",
    "    start_time = time.time()\n",
    "    faqs_data = pd.read_csv(r\"C:\\Users\\nilsh\\my_projects\\ExpoSeq\\my_experiments\\max_new\\sequencing_report.csv\").head(no_sequences)\n",
    "    if region_name != None:\n",
    "        sequences = faqs_data[region_name].tolist()\n",
    "    else:\n",
    "        sequences = faqs_data[\"targetSequences\"].apply(translate_nucleotide_to_amino_acid).tolist()\n",
    "    from transformers import RoFormerTokenizer, RoFormerModel\n",
    "    if model_path == \"alchemab/antiberta2-cssp\":\n",
    "        tokenizer = RoFormerTokenizer.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "        model = RoFormerModel.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "        Analyse = AnalyseModel(\"alchemab/antiberta2-cssp\", model, tokenizer )\n",
    "    else:\n",
    "        Analyse = AnalyseModel(model_path)\n",
    "    X = Analyse.ModelSets._get_embeddings_parallel(sequences, )\n",
    "    end_time = time.time()\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    return elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_process(r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\", 1000, region_name = None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = time_process(r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\", 1000, region_name = None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"alchemab/antiberta2-cssp\", r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\", r\"facebook/esm2_t6_8M_UR50D\", r\"facebook/esm2_t30_150M_UR50D\"]\n",
    "no_seq = 1000\n",
    "all_times = []\n",
    "region = None\n",
    "folds = 5\n",
    "for model in models:\n",
    "    fold_model = []\n",
    "    for fold in range(folds):\n",
    "        elapsed_time = time_process(model, no_seq, region_name = None )\n",
    "        fold_model.append(elapsed_time)\n",
    "    all_times.append(fold_model)\n",
    "\n",
    "means = [np.mean(sample_data) for sample_data in all_times]\n",
    "std_devs = [np.std(sample_data) for sample_data in all_times]\n",
    "\n",
    "names = [\"Antiberta2\", \"Self-trained model\", \"Esm-2b with 6 layers\", \"Esm-2b with 30 layers\"]\n",
    "assert len(names) == len(models), \"The number of models and names must be the same\"\n",
    "Analyse = AnalyseModel(r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\")\n",
    "Analyse.make_figure()\n",
    "Analyse.ax.bar(names, means, yerr = std_devs, capsize = 5, color = \"skyblue\", alpha = 0.7)\n",
    "Analyse.update_plot()\n",
    "Analyse.ax.set_ylabel(\"Time in seconds\", **Analyse.font_settings)\n",
    "Analyse.ax.set_xlabel(\"Model\",  **Analyse.font_settings)\n",
    "Analyse.ax.set_title(\"Computing time for embedding of 1000 sequences\", pad = 20, **Analyse.font_settings)\n",
    "Analyse.save_in_plots(\"time_comparison.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "means = [np.mean(sample_data) for sample_data in all_times]\n",
    "std_devs = [np.std(sample_data) for sample_data in all_times]\n",
    "print(np.mean([\n",
    "    125.92300152778625, 125.60665917396545, 124.9071409702301, 125.5185558795929]))\n",
    "print(np.std([125.92300152778625, 125.60665917396545, 124.9071409702301, 125.5185558795929]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path = r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\"\n",
    "if model_path == \"alchemab/antiberta2-cssp\":\n",
    "    from transformers import RoFormerTokenizer, RoFormerForMaskedLM\n",
    "    tokenizer = RoFormerTokenizer.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "    model = RoFormerForMaskedLM.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "    Analyse = AnalyseModel(\"alchemab/antiberta2-cssp\", model, tokenizer )     \n",
    "else:\n",
    "    Analyse = AnalyseModel(model_path, load_masked_lm=True)\n",
    "sequences = pd.read_csv(r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\data\\training_data\\val.csv\", nrows = 10000)[\"sequence\"].tolist()\n",
    "all_seqs = []\n",
    "for seq in sequences:\n",
    "    if len(seq) < 240: # there are some sequence lengths in the data that dont make sense in the context of nanobodies. 240 because there are spaces in between each tokens\n",
    "        pass\n",
    "    else:\n",
    "        all_seqs.append(seq)\n",
    "    if len(all_seqs) == 1000:\n",
    "        break\n",
    "ppl = Analyse.calculate_perplexity(all_seqs)\n",
    "print(ppl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path =  r\"facebook/esm2_t6_8M_UR50D\"\n",
    "if model_path == \"alchemab/antiberta2-cssp\":\n",
    "    from transformers import RoFormerTokenizer, RoFormerForMaskedLM\n",
    "    tokenizer = RoFormerTokenizer.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "    model = RoFormerForMaskedLM.from_pretrained(\"alchemab/antiberta2-cssp\")\n",
    "    Analyse = AnalyseModel(\"alchemab/antiberta2-cssp\", model, tokenizer )     \n",
    "else:\n",
    "    Analyse = AnalyseModel(model_path, load_masked_lm=True)\n",
    "sequences = pd.read_csv(r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\data\\training_data\\val.csv\", nrows = 1000)[\"sequence\"].tolist()\n",
    "all_seqs = []\n",
    "for seq in sequences:\n",
    "    if len(seq) < 240:\n",
    "        pass\n",
    "    else:\n",
    "        all_seqs.append(seq)\n",
    "    if len(all_seqs) == 1000:\n",
    "        break\n",
    "ppl = Analyse.calculate_perplexity(all_seqs)\n",
    "print(ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The model with lower perplexity might be heavily optimized towards predicting the next amino acid accurately, focusing narrowly on features that are directly predictive of the next outcome. This can lead to a situation where the embeddings, while effective for prediction, may not capture broader or more nuanced relationships between different types of amino acids.\n",
    "\n",
    " A reason for this can be that the model cannot distinguish between the cdr and other regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path =  r\"alchemab/antiberta2\"\n",
    "if model_path == \"alchemab/antiberta2\":\n",
    "    from transformers import RoFormerTokenizer, RoFormerForMaskedLM\n",
    "    tokenizer = RoFormerTokenizer.from_pretrained(\"alchemab/antiberta2\")\n",
    "    model = RoFormerForMaskedLM.from_pretrained(\"alchemab/antiberta2\")\n",
    "    Analyse = AnalyseModel(\"alchemab/antiberta2\", model, tokenizer )     \n",
    "else:\n",
    "    Analyse = AnalyseModel(model_path, load_masked_lm=True)\n",
    "sequences = pd.read_csv(r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\data\\training_data\\val.csv\", nrows = 1000)[\"sequence\"].tolist()\n",
    "all_seqs = []\n",
    "for seq in sequences:\n",
    "    if len(seq) < 240:\n",
    "        pass\n",
    "    else:\n",
    "        all_seqs.append(seq)\n",
    "    if len(all_seqs) == 1000:\n",
    "        break\n",
    "ppl = Analyse.calculate_perplexity(all_seqs, pad_token_id=0) # token id is 0, https://huggingface.co/alchemab/antiberta2/blob/main/vocab.txt\n",
    "print(ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path =  r\"facebook/esm2_t30_150M_UR50D\"\n",
    "if model_path == \"alchemab/antiberta2\":\n",
    "    from transformers import RoFormerTokenizer, RoFormerForMaskedLM\n",
    "    tokenizer = RoFormerTokenizer.from_pretrained(\"alchemab/antiberta2\")\n",
    "    model = RoFormerForMaskedLM.from_pretrained(\"alchemab/antiberta2\")\n",
    "    Analyse = AnalyseModel(\"alchemab/antiberta2\", model, tokenizer )     \n",
    "else:\n",
    "    Analyse = AnalyseModel(model_path, load_masked_lm=True)\n",
    "sequences = pd.read_csv(r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\data\\training_data\\val.csv\", nrows = 1000)[\"sequence\"].tolist()\n",
    "all_seqs = []\n",
    "for seq in sequences:\n",
    "    if len(seq) < 240:\n",
    "        pass\n",
    "    else:\n",
    "        all_seqs.append(seq)\n",
    "    if len(all_seqs) == 1000:\n",
    "        break\n",
    "ppl = Analyse.calculate_perplexity(all_seqs) # token id is 0, https://huggingface.co/alchemab/antiberta2/blob/main/vocab.txt\n",
    "print(ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check CDR3 positional importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nanobody_delphia.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "nanobody_delphia = pd.read_excel(r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\data\\nanobody_delphia_tidied.xlsx\")\n",
    "nanobody_delphia = nanobody_delphia.dropna(subset=['CDR3', 'Full sequence'])\n",
    "\n",
    "\n",
    "def pad_sequence(subseq, fullseq):\n",
    "    start_index = fullseq.find(subseq)\n",
    "    if start_index == -1:\n",
    "        # If subsequence not found, return padded version of the full sequence\n",
    "        return \"\"\n",
    "    \n",
    "    # Create a list of <PAD> for each character in full sequence\n",
    "    padded_sequence = ['<pad>' for _ in fullseq]\n",
    "    \n",
    "    # Replace the <PAD> tokens with the subsequence at the correct position\n",
    "    for i in range(len(subseq)):\n",
    "        padded_sequence[start_index + i] = subseq[i]\n",
    "    \n",
    "    # Join the sequence with space\n",
    "    return ' '.join(padded_sequence)\n",
    "\n",
    "# Apply the function to each row in DataFrame\n",
    "sequencing_report = pd.read_csv(r\"C:\\Users\\nilsh\\my_projects\\ExpoSeq\\my_experiments\\max_new\\sequencing_report.csv\",)\n",
    "mask = ~sequencing_report['targetSequences'].str.contains('region_not_covered')\n",
    "\n",
    "# Apply the mask to the DataFrame to keep only the rows where the condition is True\n",
    "sequencing_report = sequencing_report[mask]\n",
    "sequencing_report[\"full_seq\"] = sequencing_report[\"targetSequences\"].apply(translate_nucleotide_to_amino_acid)\n",
    "sequencing_report[\"padded_sequence\"] = sequencing_report.apply(lambda x: pad_sequence(x['aaSeqCDR3'], x['full_seq']), axis=1)\n",
    "mask = sequencing_report['padded_sequence'].str.len() > 0\n",
    "report = sequencing_report[mask]\n",
    "\n",
    "#nanobody_delphia['PaddedSequence'] = nanobody_delphia.apply(lambda x: pad_sequence(x['CDR3'], x['Full sequence']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencing_report[\"padded_sequence\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = report.groupby(\"Experiment\").head(200)\n",
    "padded_sequences = report[\"padded_sequence\"].tolist()\n",
    "labels = report[\"Experiment\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpadded_sequences = report[\"aaSeqCDR3\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picture_path = \"unpadded_cdr3_faqs_data_25n\"\n",
    "title_extension = \"specifically trained model\"\n",
    "model_path = r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\"\n",
    "Analyse = AnalyseModel(model_path)\n",
    "reduced_X = Analyse.embed_cluster_label(unpadded_sequences, labels, explained_variance_threshold = 0.9, n_neighbors = 25, min_dist = 0.1, alpha = 0.8, cmap = \"Set2\", title = f\"Unpadded CDR3 sequences with n = 25\", color_list = color_list)\n",
    "reduced_X.to_csv(f\"{picture_path}.csv\")\n",
    "Analyse.save_in_plots(f\"{picture_path}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nanobody_delphia.shape[0])\n",
    "nanobody_delphia[\"Ag1_modified\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequences = nanobody_delphia[\"PaddedSequence\"].tolist()\n",
    "labels = nanobody_delphia[\"Ag1_modified\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picture_path = \"position_cdr_importance\"\n",
    "title_extension = \"specifically trained model\"\n",
    "model_path = r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\"\n",
    "Analyse = AnalyseModel(model_path)\n",
    "reduced_X = Analyse.embed_cluster_label(padded_sequences, labels, explained_variance_threshold = 0.9, n_neighbors = 15, min_dist = 0.1, alpha = 0.8, cmap = \"Set2\", title = f\"Embedding of position specific padded CDR3 sequences.\")\n",
    "reduced_X.to_csv(f\"{picture_path}.csv\")\n",
    "\n",
    "Analyse.save_in_plots(f\"{picture_path}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analyse = AnalyseModel(r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\", load_masked_lm=True)\n",
    "ppl = Analyse.calculate_perplexity(padded_sequences)\n",
    "print(ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picture_path = \"position_cdr_dumped\"\n",
    "title_extension = \"specifically trained model\"\n",
    "unpadded_sequences = nanobody_delphia[\"CDR3\"].tolist()\n",
    "model_path = r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\"\n",
    "Analyse = AnalyseModel(model_path)\n",
    "reduced_X = Analyse.embed_cluster_label(unpadded_sequences, labels, explained_variance_threshold = 0.9, n_neighbors = 15, min_dist = 0.1, alpha = 0.8, cmap = \"Set2\", title = f\"Embedding of unpadded CDR3 sequences with {title_extension}\")\n",
    "reduced_X.to_csv(f\"{picture_path}.csv\")\n",
    "\n",
    "Analyse.save_in_plots(f\"{picture_path}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpadded_sequences = faqs_data[\"aaSeqCDR3\"].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analyse = AnalyseModel(r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\", load_masked_lm=True)\n",
    "ppl = Analyse.calculate_perplexity(unpadded_sequences)\n",
    "print(ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All complementary regions but no framework regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "nanobody_delphia = pd.read_excel(r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\data\\nanobody_delphia_tidied.xlsx\")\n",
    "nanobody_delphia = nanobody_delphia.dropna(subset=['CDR1', 'CDR2', 'CDR3', 'Full sequence'])  \n",
    "\n",
    "def pad_multiple_sequences(fullseq, *subseqs):\n",
    "    # Create a list of <PAD> for each character in full sequence\n",
    "    padded_sequence = ['<pad>' for _ in range(len(fullseq))]\n",
    "    \n",
    "    # Process each subsequence\n",
    "    for subseq in subseqs:\n",
    "        start_index = fullseq.find(subseq)\n",
    "        if start_index != -1:\n",
    "            # Replace the <PAD> tokens with the subsequence at the correct position\n",
    "            for i in range(len(subseq)):\n",
    "                padded_sequence[start_index + i] = subseq[i]\n",
    "\n",
    "    # Join the sequence with space\n",
    "    return ' '.join(padded_sequence)\n",
    "\n",
    "# Apply the function to each row in DataFrame to include CDR1, CDR2, CDR3\n",
    "nanobody_delphia['PaddedSequence'] = nanobody_delphia.apply(lambda x: pad_multiple_sequences(x['Full sequence'], x['Framework 1'], x['Framework 2'], x['Framework 3']), axis=1)\n",
    "\n",
    "# Example of saving or viewing the results\n",
    "print(nanobody_delphia[['Full sequence', 'PaddedSequence']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequences = nanobody_delphia[\"PaddedSequence\"].tolist()\n",
    "labels = nanobody_delphia[\"Ag1_modified\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picture_path = \"position_all_fr_importance\"\n",
    "title_extension = \"specifically trained model\"\n",
    "model_path = r\"C:\\Users\\nilsh\\my_projects\\SeqLP\\tests\\test_data\\nanobody_model\"\n",
    "Analyse = AnalyseModel(model_path)\n",
    "reduced_X = Analyse.embed_cluster_label(padded_sequences, labels, explained_variance_threshold = 0.9, n_neighbors = 15, min_dist = 0.1, alpha = 0.8, cmap = \"Set2\", title = f\"Embedding of position specific padded FR1, FR2 and FR3 sequences.\")\n",
    "reduced_X.to_csv(f\"{picture_path}.csv\")\n",
    "\n",
    "Analyse.save_in_plots(f\"{picture_path}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make barplot with venom composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venom_comp = pd.read_excel(r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\data\\venom_compmosition.xlsx\", index_col = 0)\n",
    "venom_comp.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_targets = venom_comp.loc[[\"Dp4\", \"Nu6\", \"Nm8\",  \"Dp8\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "legend_settings = {'loc': 'center left','facecolor': 'black',  'bbox_to_anchor': (1, 0.5), 'ncols': 1, 'fontsize': 16, 'frameon': True, 'framealpha': 1, 'facecolor': 'white', 'mode': None, 'title_fontsize': 'large', 'title_fontsize': 'large'}\n",
    "# Creating the stacked bar plot\n",
    "Analyse.make_figure()\n",
    "\n",
    "ax = specific_targets.plot(kind='bar', stacked=True, figsize=(10, 7), colormap= \"tab10\", alpha = 0.9, ax = Analyse.ax)\n",
    "label_settings = {'fontfamily': 'serif',\n",
    " 'fontsize': '14',\n",
    " 'fontstyle': 'normal',\n",
    " }\n",
    "# Adding labels and title\n",
    "plt.xlabel('Venom', **Analyse.font_settings_normal)\n",
    "plt.ylabel('Composition (%)', **Analyse.font_settings_normal)\n",
    "plt.title('Toxin Composition by Venom Type', **Analyse.font_settings_title)\n",
    "plt.xticks(rotation=0)\n",
    "# Show legend and plot\n",
    "plt.legend(title='Toxin Types', **legend_settings)\n",
    "Analyse.update_plot()\n",
    "Analyse.save_in_plots(\"venom_composition.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analyse.font_settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating with machine learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencing_report[\"Experiment\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Data = DataPipeline(no_sequences = 1000000)\n",
    "y = Data.init_sequencing_report['Experiment'].tolist()\n",
    "y_encoded = [0 for item in y if item == \"cLNTX_non-bind\"]\n",
    "y_encoded = [1 for item in y if item == ]\n",
    "ML = SupervisedML(Data.X, y_encoded, cv_components = 5)\n",
    "model = ML.logistic_regression()\n",
    "scores = ML.do_scikits_cv(model)\n",
    "ML.do_nn_cv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = DataPipeline(no_sequences = 1000000, model = esm_small)\n",
    "y = Data.init_sequencing_report['Experiment'].tolist()\n",
    "y_encoded = [0 if item == \"cLNTX_non-bind\" else 1 for item in y]\n",
    "ML = SupervisedML(Data.X, y_encoded, cv_components = 5)\n",
    "model = ML.logistic_regression()\n",
    "scores = ML.do_scikits_cv(model)\n",
    "ML.do_nn_cv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "three class problem: cLNTX ++ and aBGTX-+ | cLNTX +- aBGTX +- | Non bing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at c:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\models\\t12_320_lastlayer_2106_seqs and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\nilsh\\my_projects\\SeqLP\\src\\seqlp\\visualize\\load_model.py:237: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sequencing_report[['full_sequence', 'CDRPositions']] = sequencing_report.apply(ExtractData.calculate_cdr_positions, axis=1, result_type='expand')\n",
      "c:\\Users\\nilsh\\my_projects\\SeqLP\\src\\seqlp\\visualize\\load_model.py:237: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sequencing_report[['full_sequence', 'CDRPositions']] = sequencing_report.apply(ExtractData.calculate_cdr_positions, axis=1, result_type='expand')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance after reducing to 10 dimensions:0.9331613779067993\n"
     ]
    }
   ],
   "source": [
    "chosen_columns = [\"cLNTX_non-bind\", \"cLNTX_++\", \"aBGTX_-+\", \"cLNTX_+-\", \"aBGTX_+-\"]\n",
    "model_big = r\"c:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\models\\t12_320_lastlayer_2106_seqs\"\n",
    "Data = DataPipeline(no_sequences = 1000000, model = model_big, choose_labels= chosen_columns)\n",
    "y = Data.init_sequencing_report['Experiment'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated scores: [0.68728985 0.69804976 0.67720242 0.67585743 0.68034993]\n",
      "Average accuracy: 0.6837498789418569\n",
      "FOLD 0\n",
      "-------------------------------\n",
      "Epoch 1\n",
      "Validation: Avg loss: 0.9348, Accuracy: 0.5676\n",
      "---------------------------------\n",
      "Epoch 2\n",
      "Validation: Avg loss: 0.8784, Accuracy: 0.6153\n",
      "---------------------------------\n",
      "Epoch 3\n",
      "Validation: Avg loss: 0.8414, Accuracy: 0.6429\n",
      "---------------------------------\n",
      "Epoch 4\n",
      "Validation: Avg loss: 0.8168, Accuracy: 0.6658\n",
      "---------------------------------\n",
      "Epoch 5\n",
      "Validation: Avg loss: 0.7995, Accuracy: 0.6806\n",
      "---------------------------------\n",
      "Epoch 6\n",
      "Validation: Avg loss: 0.7872, Accuracy: 0.6886\n",
      "---------------------------------\n",
      "Epoch 7\n",
      "Validation: Avg loss: 0.7778, Accuracy: 0.7088\n",
      "---------------------------------\n",
      "Epoch 8\n",
      "Validation: Avg loss: 0.7704, Accuracy: 0.7270\n",
      "---------------------------------\n",
      "Epoch 9\n",
      "Validation: Avg loss: 0.7645, Accuracy: 0.7323\n",
      "---------------------------------\n",
      "Epoch 10\n",
      "Validation: Avg loss: 0.7601, Accuracy: 0.7350\n",
      "---------------------------------\n",
      "FOLD 1\n",
      "-------------------------------\n",
      "Epoch 1\n",
      "Validation: Avg loss: 0.9350, Accuracy: 0.5508\n",
      "---------------------------------\n",
      "Epoch 2\n",
      "Validation: Avg loss: 0.8715, Accuracy: 0.6429\n",
      "---------------------------------\n",
      "Epoch 3\n",
      "Validation: Avg loss: 0.8318, Accuracy: 0.7088\n",
      "---------------------------------\n",
      "Epoch 4\n",
      "Validation: Avg loss: 0.8041, Accuracy: 0.7283\n",
      "---------------------------------\n",
      "Epoch 5\n",
      "Validation: Avg loss: 0.7840, Accuracy: 0.7377\n",
      "---------------------------------\n",
      "Epoch 6\n",
      "Validation: Avg loss: 0.7679, Accuracy: 0.7418\n",
      "---------------------------------\n",
      "Epoch 7\n",
      "Validation: Avg loss: 0.7553, Accuracy: 0.7485\n",
      "---------------------------------\n",
      "Epoch 8\n",
      "Validation: Avg loss: 0.7463, Accuracy: 0.7687\n",
      "---------------------------------\n",
      "Epoch 9\n",
      "Validation: Avg loss: 0.7399, Accuracy: 0.7687\n",
      "---------------------------------\n",
      "Epoch 10\n",
      "Validation: Avg loss: 0.7351, Accuracy: 0.7693\n",
      "---------------------------------\n",
      "FOLD 2\n",
      "-------------------------------\n",
      "Epoch 1\n",
      "Validation: Avg loss: 0.9370, Accuracy: 0.6194\n",
      "---------------------------------\n",
      "Epoch 2\n",
      "Validation: Avg loss: 0.8602, Accuracy: 0.6557\n",
      "---------------------------------\n",
      "Epoch 3\n",
      "Validation: Avg loss: 0.8166, Accuracy: 0.6779\n",
      "---------------------------------\n",
      "Epoch 4\n",
      "Validation: Avg loss: 0.7913, Accuracy: 0.7034\n",
      "---------------------------------\n",
      "Epoch 5\n",
      "Validation: Avg loss: 0.7759, Accuracy: 0.7108\n",
      "---------------------------------\n",
      "Epoch 6\n",
      "Validation: Avg loss: 0.7666, Accuracy: 0.7182\n",
      "---------------------------------\n",
      "Epoch 7\n",
      "Validation: Avg loss: 0.7604, Accuracy: 0.7169\n",
      "---------------------------------\n",
      "Epoch 8\n",
      "Validation: Avg loss: 0.7560, Accuracy: 0.7189\n",
      "---------------------------------\n",
      "Epoch 9\n",
      "Validation: Avg loss: 0.7528, Accuracy: 0.7196\n",
      "---------------------------------\n",
      "Epoch 10\n",
      "Validation: Avg loss: 0.7503, Accuracy: 0.7182\n",
      "---------------------------------\n",
      "FOLD 3\n",
      "-------------------------------\n",
      "Epoch 1\n",
      "Validation: Avg loss: 0.9871, Accuracy: 0.4580\n",
      "---------------------------------\n",
      "Epoch 2\n",
      "Validation: Avg loss: 0.9118, Accuracy: 0.5488\n",
      "---------------------------------\n",
      "Epoch 3\n",
      "Validation: Avg loss: 0.8651, Accuracy: 0.6268\n",
      "---------------------------------\n",
      "Epoch 4\n",
      "Validation: Avg loss: 0.8348, Accuracy: 0.6530\n",
      "---------------------------------\n",
      "Epoch 5\n",
      "Validation: Avg loss: 0.8129, Accuracy: 0.6698\n",
      "---------------------------------\n",
      "Epoch 6\n",
      "Validation: Avg loss: 0.7968, Accuracy: 0.6900\n",
      "---------------------------------\n",
      "Epoch 7\n",
      "Validation: Avg loss: 0.7848, Accuracy: 0.6947\n",
      "---------------------------------\n",
      "Epoch 8\n",
      "Validation: Avg loss: 0.7757, Accuracy: 0.6967\n",
      "---------------------------------\n",
      "Epoch 9\n",
      "Validation: Avg loss: 0.7686, Accuracy: 0.7001\n",
      "---------------------------------\n",
      "Epoch 10\n",
      "Validation: Avg loss: 0.7631, Accuracy: 0.7061\n",
      "---------------------------------\n",
      "FOLD 4\n",
      "-------------------------------\n",
      "Epoch 1\n",
      "Validation: Avg loss: 0.9453, Accuracy: 0.4852\n",
      "---------------------------------\n",
      "Epoch 2\n",
      "Validation: Avg loss: 0.8834, Accuracy: 0.5794\n",
      "---------------------------------\n",
      "Epoch 3\n",
      "Validation: Avg loss: 0.8454, Accuracy: 0.6359\n",
      "---------------------------------\n",
      "Epoch 4\n",
      "Validation: Avg loss: 0.8185, Accuracy: 0.6541\n",
      "---------------------------------\n",
      "Epoch 5\n",
      "Validation: Avg loss: 0.7978, Accuracy: 0.6696\n",
      "---------------------------------\n",
      "Epoch 6\n",
      "Validation: Avg loss: 0.7819, Accuracy: 0.6837\n",
      "---------------------------------\n",
      "Epoch 7\n",
      "Validation: Avg loss: 0.7702, Accuracy: 0.6911\n",
      "---------------------------------\n",
      "Epoch 8\n",
      "Validation: Avg loss: 0.7618, Accuracy: 0.6978\n",
      "---------------------------------\n",
      "Epoch 9\n",
      "Validation: Avg loss: 0.7560, Accuracy: 0.6965\n",
      "---------------------------------\n",
      "Epoch 10\n",
      "Validation: Avg loss: 0.7521, Accuracy: 0.7012\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_encoded = [0 if item == \"cLNTX_non-bind\" else 1 if item in ['cLNTX_++', 'aBGTX_-+'] else 2 if item in ['cLNTX_+-', 'aBGTX_+-'] else item for item in y]\n",
    "\n",
    "ML = SupervisedML(Data.X, y_encoded, cv_components = 5)\n",
    "model = ML.logistic_regression()\n",
    "scores = ML.do_scikits_cv(model)\n",
    "ML.do_nn_cv(num_epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\models\\t6_320_lastlayer_5106_seqs and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\nilsh\\my_projects\\SeqLP\\src\\seqlp\\visualize\\load_model.py:237: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sequencing_report[['full_sequence', 'CDRPositions']] = sequencing_report.apply(ExtractData.calculate_cdr_positions, axis=1, result_type='expand')\n",
      "c:\\Users\\nilsh\\my_projects\\SeqLP\\src\\seqlp\\visualize\\load_model.py:237: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sequencing_report[['full_sequence', 'CDRPositions']] = sequencing_report.apply(ExtractData.calculate_cdr_positions, axis=1, result_type='expand')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance after reducing to 10 dimensions:0.918671190738678\n",
      "Cross-validated scores: [0.67518494 0.65232011 0.65030262 0.64223268 0.66016151]\n",
      "Average accuracy: 0.65604037141996\n",
      "FOLD 0\n",
      "-------------------------------\n",
      "Epoch 1\n",
      "Validation: Avg loss: 0.9577, Accuracy: 0.5649\n",
      "---------------------------------\n",
      "Epoch 2\n",
      "Validation: Avg loss: 0.8983, Accuracy: 0.6066\n",
      "---------------------------------\n",
      "Epoch 3\n",
      "Validation: Avg loss: 0.8548, Accuracy: 0.6557\n",
      "---------------------------------\n",
      "Epoch 4\n",
      "Validation: Avg loss: 0.8230, Accuracy: 0.6893\n",
      "---------------------------------\n",
      "Epoch 5\n",
      "Validation: Avg loss: 0.7992, Accuracy: 0.7021\n",
      "---------------------------------\n",
      "Epoch 6\n",
      "Validation: Avg loss: 0.7812, Accuracy: 0.7155\n",
      "---------------------------------\n",
      "Epoch 7\n",
      "Validation: Avg loss: 0.7682, Accuracy: 0.7209\n",
      "---------------------------------\n",
      "Epoch 8\n",
      "Validation: Avg loss: 0.7585, Accuracy: 0.7249\n",
      "---------------------------------\n",
      "Epoch 9\n",
      "Validation: Avg loss: 0.7507, Accuracy: 0.7344\n",
      "---------------------------------\n",
      "Epoch 10\n",
      "Validation: Avg loss: 0.7445, Accuracy: 0.7357\n",
      "---------------------------------\n",
      "FOLD 1\n",
      "-------------------------------\n",
      "Epoch 1\n",
      "Validation: Avg loss: 0.9252, Accuracy: 0.6227\n",
      "---------------------------------\n",
      "Epoch 2\n",
      "Validation: Avg loss: 0.8752, Accuracy: 0.6638\n",
      "---------------------------------\n",
      "Epoch 3\n",
      "Validation: Avg loss: 0.8476, Accuracy: 0.6994\n",
      "---------------------------------\n",
      "Epoch 4\n",
      "Validation: Avg loss: 0.8291, Accuracy: 0.7149\n",
      "---------------------------------\n",
      "Epoch 5\n",
      "Validation: Avg loss: 0.8151, Accuracy: 0.7209\n",
      "---------------------------------\n",
      "Epoch 6\n",
      "Validation: Avg loss: 0.8037, Accuracy: 0.7290\n",
      "---------------------------------\n",
      "Epoch 7\n",
      "Validation: Avg loss: 0.7943, Accuracy: 0.7310\n",
      "---------------------------------\n",
      "Epoch 8\n",
      "Validation: Avg loss: 0.7870, Accuracy: 0.7330\n",
      "---------------------------------\n",
      "Epoch 9\n",
      "Validation: Avg loss: 0.7812, Accuracy: 0.7377\n",
      "---------------------------------\n",
      "Epoch 10\n",
      "Validation: Avg loss: 0.7772, Accuracy: 0.7404\n",
      "---------------------------------\n",
      "FOLD 2\n",
      "-------------------------------\n",
      "Epoch 1\n",
      "Validation: Avg loss: 0.9250, Accuracy: 0.5696\n",
      "---------------------------------\n",
      "Epoch 2\n",
      "Validation: Avg loss: 0.8746, Accuracy: 0.6066\n",
      "---------------------------------\n",
      "Epoch 3\n",
      "Validation: Avg loss: 0.8472, Accuracy: 0.6355\n",
      "---------------------------------\n",
      "Epoch 4\n",
      "Validation: Avg loss: 0.8285, Accuracy: 0.6638\n",
      "---------------------------------\n",
      "Epoch 5\n",
      "Validation: Avg loss: 0.8156, Accuracy: 0.6691\n",
      "---------------------------------\n",
      "Epoch 6\n",
      "Validation: Avg loss: 0.8055, Accuracy: 0.6779\n",
      "---------------------------------\n",
      "Epoch 7\n",
      "Validation: Avg loss: 0.7969, Accuracy: 0.6752\n",
      "---------------------------------\n",
      "Epoch 8\n",
      "Validation: Avg loss: 0.7899, Accuracy: 0.6759\n",
      "---------------------------------\n",
      "Epoch 9\n",
      "Validation: Avg loss: 0.7841, Accuracy: 0.6745\n",
      "---------------------------------\n",
      "Epoch 10\n",
      "Validation: Avg loss: 0.7790, Accuracy: 0.6732\n",
      "---------------------------------\n",
      "FOLD 3\n",
      "-------------------------------\n",
      "Epoch 1\n",
      "Validation: Avg loss: 0.9295, Accuracy: 0.5999\n",
      "---------------------------------\n",
      "Epoch 2\n",
      "Validation: Avg loss: 0.8555, Accuracy: 0.6537\n",
      "---------------------------------\n",
      "Epoch 3\n",
      "Validation: Avg loss: 0.8156, Accuracy: 0.6685\n",
      "---------------------------------\n",
      "Epoch 4\n",
      "Validation: Avg loss: 0.7934, Accuracy: 0.6833\n",
      "---------------------------------\n",
      "Epoch 5\n",
      "Validation: Avg loss: 0.7799, Accuracy: 0.6886\n",
      "---------------------------------\n",
      "Epoch 6\n",
      "Validation: Avg loss: 0.7703, Accuracy: 0.6927\n",
      "---------------------------------\n",
      "Epoch 7\n",
      "Validation: Avg loss: 0.7631, Accuracy: 0.6940\n",
      "---------------------------------\n",
      "Epoch 8\n",
      "Validation: Avg loss: 0.7571, Accuracy: 0.6987\n",
      "---------------------------------\n",
      "Epoch 9\n",
      "Validation: Avg loss: 0.7517, Accuracy: 0.6980\n",
      "---------------------------------\n",
      "Epoch 10\n",
      "Validation: Avg loss: 0.7467, Accuracy: 0.6980\n",
      "---------------------------------\n",
      "FOLD 4\n",
      "-------------------------------\n",
      "Epoch 1\n",
      "Validation: Avg loss: 0.9956, Accuracy: 0.4838\n",
      "---------------------------------\n",
      "Epoch 2\n",
      "Validation: Avg loss: 0.9161, Accuracy: 0.5747\n",
      "---------------------------------\n",
      "Epoch 3\n",
      "Validation: Avg loss: 0.8726, Accuracy: 0.6104\n",
      "---------------------------------\n",
      "Epoch 4\n",
      "Validation: Avg loss: 0.8432, Accuracy: 0.6353\n",
      "---------------------------------\n",
      "Epoch 5\n",
      "Validation: Avg loss: 0.8238, Accuracy: 0.6528\n",
      "---------------------------------\n",
      "Epoch 6\n",
      "Validation: Avg loss: 0.8097, Accuracy: 0.6615\n",
      "---------------------------------\n",
      "Epoch 7\n",
      "Validation: Avg loss: 0.8001, Accuracy: 0.6723\n",
      "---------------------------------\n",
      "Epoch 8\n",
      "Validation: Avg loss: 0.7928, Accuracy: 0.6770\n",
      "---------------------------------\n",
      "Epoch 9\n",
      "Validation: Avg loss: 0.7883, Accuracy: 0.6750\n",
      "---------------------------------\n",
      "Epoch 10\n",
      "Validation: Avg loss: 0.7851, Accuracy: 0.6783\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "chosen_columns = [\"cLNTX_non-bind\", \"cLNTX_++\", \"aBGTX_-+\", \"cLNTX_+-\", \"aBGTX_+-\"]\n",
    "Data = DataPipeline(no_sequences = 1000000, model = r\"C:\\Users\\nilsh\\OneDrive\\Desktop\\results_thesis\\models\\t6_320_lastlayer_5106_seqs\", choose_labels= chosen_columns)\n",
    "y = Data.init_sequencing_report['Experiment'].tolist()\n",
    "y_encoded = [0 if item == \"cLNTX_non-bind\" else 1 if item in ['cLNTX_++', 'aBGTX_-+'] else 2 if item in ['cLNTX_+-', 'aBGTX_+-'] else item for item in y]\n",
    "\n",
    "ML = SupervisedML(Data.X, y_encoded, cv_components = 5)\n",
    "model = ML.logistic_regression()\n",
    "scores = ML.do_scikits_cv(model)\n",
    "ML.do_nn_cv(num_epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unequal length arrays",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31316\\142547103.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mesm_accuracies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.7371\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.7128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.7041\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.6622\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrained_model_accuracies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.7012\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.7061\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.7182\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.7693\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.7350\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mAnalyseModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpaired_t_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mesm_accuracies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrained_model_accuracies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\src\\seqlp\\use_model.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model_A_accuracies, model_B_accuracies)\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpaired_t_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_A_accuracies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_B_accuracies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[0mdifferences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_A_accuracies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_B_accuracies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m         \u001b[0mt_statistic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mttest_rel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_A_accuracies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_B_accuracies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"T-statistic: {t_statistic}, P-value: {p_value}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    527\u001b[0m                 \u001b[1;31m# behavior of those would break backward compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msentinel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m                     \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_remove_sentinel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpaired\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentinel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhypotest_fun_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult_to_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_add_reduced_axes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduced_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mtuple_to_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nilsh\\my_projects\\SeqLP\\.venv\\Lib\\site-packages\\scipy\\stats\\_stats_py.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(a, b, axis, nan_policy, alternative)\u001b[0m\n\u001b[0;32m   7740\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7741\u001b[0m     \u001b[0mna\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_len\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"first argument\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7742\u001b[0m     \u001b[0mnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_len\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"second argument\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7743\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mna\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7744\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unequal length arrays'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7745\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7746\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mna\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnb\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7747\u001b[0m         \u001b[1;31m# _axis_nan_policy decorator ensures this only happens with 1d input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: unequal length arrays"
     ]
    }
   ],
   "source": [
    "esm_accuracies = [0.7371, 0.7128, 0.7041, 0.6622]\n",
    "trained_model_accuracies = [0.7012, 0.7061,0.7182, 0.7693, 0.7350]\n",
    "AnalyseModel.paired_t_test(esm_accuracies, trained_model_accuracies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
